{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1."
      ],
      "metadata": {
        "id": "Qt_zLhUPf5x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i."
      ],
      "metadata": {
        "id": "BmTh3VZjf712"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.arange(0,31)\n",
        "Y = np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41, 40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])"
      ],
      "metadata": {
        "id": "d2vZ0UrqgQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "X_tensor = torch.tensor(X)\n",
        "Y_tensor = torch.tensor(Y)"
      ],
      "metadata": {
        "id": "bmd7KSWZ-u8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii."
      ],
      "metadata": {
        "id": "7COCMeAei88c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_1_X = torch.unsqueeze(X_tensor[0:20], 1)\n",
        "dataset_train_1_Y = torch.unsqueeze(Y_tensor[0:20], 1)\n",
        "dataset_test_1_X = torch.unsqueeze(X_tensor[20:31], 1)\n",
        "dataset_test_1_Y = torch.unsqueeze(Y_tensor[20:31], 1)"
      ],
      "metadata": {
        "id": "43nZf_GRi9dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrDyMddUfMp8"
      },
      "outputs": [],
      "source": [
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "      x = dataset_train_1_X.to(dtype=torch.float32)\n",
        "      y = dataset_train_1_Y.to(dtype=torch.float32)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(x)\n",
        "      loss = criterion(outputs, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "        \n",
        "\n",
        "      print ('Epch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, 1, len(dataset_train_1_Y), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    total = 0.0\n",
        "    x = dataset_test_1_X.float()\n",
        "    y = dataset_test_1_Y.float()\n",
        "    outputs = net(x)\n",
        "    MSE = (outputs - y)**2\n",
        "    total += MSE\n",
        "        \n",
        "    print('MSE Error: {}'.format(torch.mean(total)))"
      ],
      "metadata": {
        "id": "EgJwBLtCjs6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945adc6b-ee3f-4f90-8d50-8569c2c5bff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epch [1/100], Step [1/20], Loss: 1486.4567\n",
            "Epch [2/100], Step [1/20], Loss: 1055.8335\n",
            "Epch [3/100], Step [1/20], Loss: 753.4221\n",
            "Epch [4/100], Step [1/20], Loss: 485.4915\n",
            "Epch [5/100], Step [1/20], Loss: 307.4189\n",
            "Epch [6/100], Step [1/20], Loss: 288.8363\n",
            "Epch [7/100], Step [1/20], Loss: 413.0146\n",
            "Epch [8/100], Step [1/20], Loss: 495.1708\n",
            "Epch [9/100], Step [1/20], Loss: 465.6107\n",
            "Epch [10/100], Step [1/20], Loss: 380.1458\n",
            "Epch [11/100], Step [1/20], Loss: 300.3297\n",
            "Epch [12/100], Step [1/20], Loss: 256.5959\n",
            "Epch [13/100], Step [1/20], Loss: 249.5661\n",
            "Epch [14/100], Step [1/20], Loss: 262.7307\n",
            "Epch [15/100], Step [1/20], Loss: 279.6632\n",
            "Epch [16/100], Step [1/20], Loss: 289.1739\n",
            "Epch [17/100], Step [1/20], Loss: 286.4951\n",
            "Epch [18/100], Step [1/20], Loss: 272.2354\n",
            "Epch [19/100], Step [1/20], Loss: 250.4577\n",
            "Epch [20/100], Step [1/20], Loss: 227.2456\n",
            "Epch [21/100], Step [1/20], Loss: 209.0825\n",
            "Epch [22/100], Step [1/20], Loss: 200.7581\n",
            "Epch [23/100], Step [1/20], Loss: 202.3980\n",
            "Epch [24/100], Step [1/20], Loss: 206.6893\n",
            "Epch [25/100], Step [1/20], Loss: 204.4802\n",
            "Epch [26/100], Step [1/20], Loss: 193.4422\n",
            "Epch [27/100], Step [1/20], Loss: 176.1991\n",
            "Epch [28/100], Step [1/20], Loss: 158.5525\n",
            "Epch [29/100], Step [1/20], Loss: 144.7765\n",
            "Epch [30/100], Step [1/20], Loss: 135.9185\n",
            "Epch [31/100], Step [1/20], Loss: 129.9846\n",
            "Epch [32/100], Step [1/20], Loss: 124.1895\n",
            "Epch [33/100], Step [1/20], Loss: 115.5630\n",
            "Epch [34/100], Step [1/20], Loss: 104.2573\n",
            "Epch [35/100], Step [1/20], Loss: 92.1899\n",
            "Epch [36/100], Step [1/20], Loss: 82.0797\n",
            "Epch [37/100], Step [1/20], Loss: 75.7540\n",
            "Epch [38/100], Step [1/20], Loss: 71.3943\n",
            "Epch [39/100], Step [1/20], Loss: 64.3305\n",
            "Epch [40/100], Step [1/20], Loss: 54.3288\n",
            "Epch [41/100], Step [1/20], Loss: 44.3092\n",
            "Epch [42/100], Step [1/20], Loss: 37.5560\n",
            "Epch [43/100], Step [1/20], Loss: 33.5875\n",
            "Epch [44/100], Step [1/20], Loss: 29.5246\n",
            "Epch [45/100], Step [1/20], Loss: 24.1877\n",
            "Epch [46/100], Step [1/20], Loss: 18.5572\n",
            "Epch [47/100], Step [1/20], Loss: 14.6085\n",
            "Epch [48/100], Step [1/20], Loss: 13.9379\n",
            "Epch [49/100], Step [1/20], Loss: 13.5599\n",
            "Epch [50/100], Step [1/20], Loss: 11.5270\n",
            "Epch [51/100], Step [1/20], Loss: 10.1027\n",
            "Epch [52/100], Step [1/20], Loss: 10.8263\n",
            "Epch [53/100], Step [1/20], Loss: 12.2430\n",
            "Epch [54/100], Step [1/20], Loss: 12.6634\n",
            "Epch [55/100], Step [1/20], Loss: 12.5831\n",
            "Epch [56/100], Step [1/20], Loss: 13.1109\n",
            "Epch [57/100], Step [1/20], Loss: 13.7551\n",
            "Epch [58/100], Step [1/20], Loss: 13.3120\n",
            "Epch [59/100], Step [1/20], Loss: 12.2835\n",
            "Epch [60/100], Step [1/20], Loss: 11.7468\n",
            "Epch [61/100], Step [1/20], Loss: 11.3405\n",
            "Epch [62/100], Step [1/20], Loss: 10.3664\n",
            "Epch [63/100], Step [1/20], Loss: 9.2327\n",
            "Epch [64/100], Step [1/20], Loss: 8.6243\n",
            "Epch [65/100], Step [1/20], Loss: 8.2902\n",
            "Epch [66/100], Step [1/20], Loss: 7.7268\n",
            "Epch [67/100], Step [1/20], Loss: 7.2147\n",
            "Epch [68/100], Step [1/20], Loss: 7.1158\n",
            "Epch [69/100], Step [1/20], Loss: 7.1741\n",
            "Epch [70/100], Step [1/20], Loss: 7.0652\n",
            "Epch [71/100], Step [1/20], Loss: 6.9416\n",
            "Epch [72/100], Step [1/20], Loss: 7.0230\n",
            "Epch [73/100], Step [1/20], Loss: 7.1549\n",
            "Epch [74/100], Step [1/20], Loss: 7.1224\n",
            "Epch [75/100], Step [1/20], Loss: 7.0260\n",
            "Epch [76/100], Step [1/20], Loss: 7.0230\n",
            "Epch [77/100], Step [1/20], Loss: 7.0263\n",
            "Epch [78/100], Step [1/20], Loss: 6.9073\n",
            "Epch [79/100], Step [1/20], Loss: 6.7429\n",
            "Epch [80/100], Step [1/20], Loss: 6.6434\n",
            "Epch [81/100], Step [1/20], Loss: 6.5342\n",
            "Epch [82/100], Step [1/20], Loss: 6.2990\n",
            "Epch [83/100], Step [1/20], Loss: 6.0349\n",
            "Epch [84/100], Step [1/20], Loss: 5.9309\n",
            "Epch [85/100], Step [1/20], Loss: 5.8721\n",
            "Epch [86/100], Step [1/20], Loss: 5.8022\n",
            "Epch [87/100], Step [1/20], Loss: 5.7394\n",
            "Epch [88/100], Step [1/20], Loss: 5.7057\n",
            "Epch [89/100], Step [1/20], Loss: 5.6797\n",
            "Epch [90/100], Step [1/20], Loss: 5.6471\n",
            "Epch [91/100], Step [1/20], Loss: 5.6274\n",
            "Epch [92/100], Step [1/20], Loss: 5.6213\n",
            "Epch [93/100], Step [1/20], Loss: 5.6078\n",
            "Epch [94/100], Step [1/20], Loss: 5.5896\n",
            "Epch [95/100], Step [1/20], Loss: 5.5794\n",
            "Epch [96/100], Step [1/20], Loss: 5.5672\n",
            "Epch [97/100], Step [1/20], Loss: 5.5445\n",
            "Epch [98/100], Step [1/20], Loss: 5.5228\n",
            "Epch [99/100], Step [1/20], Loss: 5.5042\n",
            "Epch [100/100], Step [1/20], Loss: 5.4793\n",
            "MSE Error: 5.165465354919434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii."
      ],
      "metadata": {
        "id": "ZDFO5PoENCS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_1_X = torch.unsqueeze(torch.cat((X_tensor[0:10], X_tensor[21:31])), 1)\n",
        "dataset_train_1_Y = torch.unsqueeze(torch.cat((Y_tensor[0:10], Y_tensor[21:31])), 1)\n",
        "dataset_test_1_X = torch.unsqueeze(X_tensor[10:21], 1)\n",
        "dataset_test_1_Y = torch.unsqueeze(Y_tensor[10:21], 1)"
      ],
      "metadata": {
        "id": "eHc2N8ynNDJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "      x = dataset_train_1_X.to(dtype=torch.float32)\n",
        "      y = dataset_train_1_Y.to(dtype=torch.float32)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(x)\n",
        "      loss = criterion(outputs, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "        \n",
        "\n",
        "      print ('Epch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, 1, len(dataset_train_1_Y), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    total = 0.0\n",
        "    x = dataset_test_1_X.float()\n",
        "    y = dataset_test_1_Y.float()\n",
        "    outputs = net(x)\n",
        "    MSE = (outputs - y)**2\n",
        "    total += MSE\n",
        "        \n",
        "    print('MSE Error: {}'.format(torch.mean(total)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfdqC9x6Nqhh",
        "outputId": "a7e72e29-a456-4cb5-bd5d-56b1cdbee137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epch [1/100], Step [1/20], Loss: 1769.0496\n",
            "Epch [2/100], Step [1/20], Loss: 825.3699\n",
            "Epch [3/100], Step [1/20], Loss: 376.2801\n",
            "Epch [4/100], Step [1/20], Loss: 465.3051\n",
            "Epch [5/100], Step [1/20], Loss: 658.8234\n",
            "Epch [6/100], Step [1/20], Loss: 562.4976\n",
            "Epch [7/100], Step [1/20], Loss: 401.2359\n",
            "Epch [8/100], Step [1/20], Loss: 327.0901\n",
            "Epch [9/100], Step [1/20], Loss: 343.1254\n",
            "Epch [10/100], Step [1/20], Loss: 391.9789\n",
            "Epch [11/100], Step [1/20], Loss: 425.2659\n",
            "Epch [12/100], Step [1/20], Loss: 425.9852\n",
            "Epch [13/100], Step [1/20], Loss: 397.5020\n",
            "Epch [14/100], Step [1/20], Loss: 354.0180\n",
            "Epch [15/100], Step [1/20], Loss: 313.8083\n",
            "Epch [16/100], Step [1/20], Loss: 292.5163\n",
            "Epch [17/100], Step [1/20], Loss: 295.4803\n",
            "Epch [18/100], Step [1/20], Loss: 312.4780\n",
            "Epch [19/100], Step [1/20], Loss: 323.7015\n",
            "Epch [20/100], Step [1/20], Loss: 315.1230\n",
            "Epch [21/100], Step [1/20], Loss: 289.8816\n",
            "Epch [22/100], Step [1/20], Loss: 261.5341\n",
            "Epch [23/100], Step [1/20], Loss: 241.9224\n",
            "Epch [24/100], Step [1/20], Loss: 233.7808\n",
            "Epch [25/100], Step [1/20], Loss: 231.6682\n",
            "Epch [26/100], Step [1/20], Loss: 227.6051\n",
            "Epch [27/100], Step [1/20], Loss: 216.3761\n",
            "Epch [28/100], Step [1/20], Loss: 198.7076\n",
            "Epch [29/100], Step [1/20], Loss: 179.8700\n",
            "Epch [30/100], Step [1/20], Loss: 166.0215\n",
            "Epch [31/100], Step [1/20], Loss: 160.5499\n",
            "Epch [32/100], Step [1/20], Loss: 154.7590\n",
            "Epch [33/100], Step [1/20], Loss: 139.1983\n",
            "Epch [34/100], Step [1/20], Loss: 118.7513\n",
            "Epch [35/100], Step [1/20], Loss: 102.1726\n",
            "Epch [36/100], Step [1/20], Loss: 91.8846\n",
            "Epch [37/100], Step [1/20], Loss: 81.0972\n",
            "Epch [38/100], Step [1/20], Loss: 64.9220\n",
            "Epch [39/100], Step [1/20], Loss: 49.6822\n",
            "Epch [40/100], Step [1/20], Loss: 42.8850\n",
            "Epch [41/100], Step [1/20], Loss: 36.9446\n",
            "Epch [42/100], Step [1/20], Loss: 23.4568\n",
            "Epch [43/100], Step [1/20], Loss: 15.8163\n",
            "Epch [44/100], Step [1/20], Loss: 16.1630\n",
            "Epch [45/100], Step [1/20], Loss: 11.3104\n",
            "Epch [46/100], Step [1/20], Loss: 7.8674\n",
            "Epch [47/100], Step [1/20], Loss: 12.9996\n",
            "Epch [48/100], Step [1/20], Loss: 14.6204\n",
            "Epch [49/100], Step [1/20], Loss: 12.7111\n",
            "Epch [50/100], Step [1/20], Loss: 17.1943\n",
            "Epch [51/100], Step [1/20], Loss: 18.4494\n",
            "Epch [52/100], Step [1/20], Loss: 16.0122\n",
            "Epch [53/100], Step [1/20], Loss: 16.9281\n",
            "Epch [54/100], Step [1/20], Loss: 16.4872\n",
            "Epch [55/100], Step [1/20], Loss: 12.8698\n",
            "Epch [56/100], Step [1/20], Loss: 11.7476\n",
            "Epch [57/100], Step [1/20], Loss: 11.2089\n",
            "Epch [58/100], Step [1/20], Loss: 8.5968\n",
            "Epch [59/100], Step [1/20], Loss: 7.1778\n",
            "Epch [60/100], Step [1/20], Loss: 7.3916\n",
            "Epch [61/100], Step [1/20], Loss: 6.5260\n",
            "Epch [62/100], Step [1/20], Loss: 5.6240\n",
            "Epch [63/100], Step [1/20], Loss: 6.1164\n",
            "Epch [64/100], Step [1/20], Loss: 6.3531\n",
            "Epch [65/100], Step [1/20], Loss: 5.8209\n",
            "Epch [66/100], Step [1/20], Loss: 5.9608\n",
            "Epch [67/100], Step [1/20], Loss: 6.4317\n",
            "Epch [68/100], Step [1/20], Loss: 6.0516\n",
            "Epch [69/100], Step [1/20], Loss: 5.7164\n",
            "Epch [70/100], Step [1/20], Loss: 5.8881\n",
            "Epch [71/100], Step [1/20], Loss: 5.5473\n",
            "Epch [72/100], Step [1/20], Loss: 5.0011\n",
            "Epch [73/100], Step [1/20], Loss: 4.9647\n",
            "Epch [74/100], Step [1/20], Loss: 4.7214\n",
            "Epch [75/100], Step [1/20], Loss: 4.1618\n",
            "Epch [76/100], Step [1/20], Loss: 4.0543\n",
            "Epch [77/100], Step [1/20], Loss: 3.9465\n",
            "Epch [78/100], Step [1/20], Loss: 3.7025\n",
            "Epch [79/100], Step [1/20], Loss: 3.7172\n",
            "Epch [80/100], Step [1/20], Loss: 3.6737\n",
            "Epch [81/100], Step [1/20], Loss: 3.5725\n",
            "Epch [82/100], Step [1/20], Loss: 3.6816\n",
            "Epch [83/100], Step [1/20], Loss: 3.6773\n",
            "Epch [84/100], Step [1/20], Loss: 3.6435\n",
            "Epch [85/100], Step [1/20], Loss: 3.7173\n",
            "Epch [86/100], Step [1/20], Loss: 3.6508\n",
            "Epch [87/100], Step [1/20], Loss: 3.6274\n",
            "Epch [88/100], Step [1/20], Loss: 3.6372\n",
            "Epch [89/100], Step [1/20], Loss: 3.5460\n",
            "Epch [90/100], Step [1/20], Loss: 3.5267\n",
            "Epch [91/100], Step [1/20], Loss: 3.4897\n",
            "Epch [92/100], Step [1/20], Loss: 3.4389\n",
            "Epch [93/100], Step [1/20], Loss: 3.4622\n",
            "Epch [94/100], Step [1/20], Loss: 3.4269\n",
            "Epch [95/100], Step [1/20], Loss: 3.4037\n",
            "Epch [96/100], Step [1/20], Loss: 3.4070\n",
            "Epch [97/100], Step [1/20], Loss: 3.3884\n",
            "Epch [98/100], Step [1/20], Loss: 3.4111\n",
            "Epch [99/100], Step [1/20], Loss: 3.4087\n",
            "Epch [100/100], Step [1/20], Loss: 3.3812\n",
            "MSE Error: 7.677873134613037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2."
      ],
      "metadata": {
        "id": "jsDp2XcommV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i."
      ],
      "metadata": {
        "id": "oPxb0BYpmpRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784')\n",
        "X = mnist.data / 255.0\n",
        "y = mnist.target\n",
        "\n",
        "y_int = np.array([int(numeric_string) for numeric_string in y])\n",
        "\n",
        "pca = PCA(n_components=20)\n",
        "X_pca = pca.fit_transform(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VLRkqm8moU6",
        "outputId": "18d0b09e-0e3d-4821-f940-7253a2ea4074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_pca.shape)\n",
        "print(y_int.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLbt2pQXoOEJ",
        "outputId": "ea43d713-14de-4a68-856a-95697a76bf3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 20)\n",
            "(70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii."
      ],
      "metadata": {
        "id": "G13i9na0neov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Now split them\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_int, test_size=(1/7))"
      ],
      "metadata": {
        "id": "JQBM2RZTnI4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train)) # create datset\n",
        "dataset_test = torch.utils.data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=64, shuffle=True) # create dataloader\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "RvopbMzVpEMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "73taKzFkrba7"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        images, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(dataloader_train), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in dataloader_test:\n",
        "        images, labels = data\n",
        "        outputs = net(images.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsZ7y_3XrJYF",
        "outputId": "e39c1c94-87e7-461c-af13-4715b887ee12"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/938], Loss: 2.2041\n",
            "Epoch [1/10], Step [200/938], Loss: 1.9616\n",
            "Epoch [1/10], Step [300/938], Loss: 1.7411\n",
            "Epoch [1/10], Step [400/938], Loss: 1.2983\n",
            "Epoch [1/10], Step [500/938], Loss: 1.3164\n",
            "Epoch [1/10], Step [600/938], Loss: 1.0768\n",
            "Epoch [1/10], Step [700/938], Loss: 0.8260\n",
            "Epoch [1/10], Step [800/938], Loss: 0.7486\n",
            "Epoch [1/10], Step [900/938], Loss: 0.8761\n",
            "Epoch [2/10], Step [100/938], Loss: 0.6182\n",
            "Epoch [2/10], Step [200/938], Loss: 0.6210\n",
            "Epoch [2/10], Step [300/938], Loss: 0.6801\n",
            "Epoch [2/10], Step [400/938], Loss: 0.3948\n",
            "Epoch [2/10], Step [500/938], Loss: 0.5762\n",
            "Epoch [2/10], Step [600/938], Loss: 0.4649\n",
            "Epoch [2/10], Step [700/938], Loss: 0.5570\n",
            "Epoch [2/10], Step [800/938], Loss: 0.3755\n",
            "Epoch [2/10], Step [900/938], Loss: 0.5313\n",
            "Epoch [3/10], Step [100/938], Loss: 0.3586\n",
            "Epoch [3/10], Step [200/938], Loss: 0.3324\n",
            "Epoch [3/10], Step [300/938], Loss: 0.4689\n",
            "Epoch [3/10], Step [400/938], Loss: 0.3831\n",
            "Epoch [3/10], Step [500/938], Loss: 0.3375\n",
            "Epoch [3/10], Step [600/938], Loss: 0.3247\n",
            "Epoch [3/10], Step [700/938], Loss: 0.3534\n",
            "Epoch [3/10], Step [800/938], Loss: 0.3610\n",
            "Epoch [3/10], Step [900/938], Loss: 0.3650\n",
            "Epoch [4/10], Step [100/938], Loss: 0.2706\n",
            "Epoch [4/10], Step [200/938], Loss: 0.3445\n",
            "Epoch [4/10], Step [300/938], Loss: 0.2191\n",
            "Epoch [4/10], Step [400/938], Loss: 0.2507\n",
            "Epoch [4/10], Step [500/938], Loss: 0.2510\n",
            "Epoch [4/10], Step [600/938], Loss: 0.5463\n",
            "Epoch [4/10], Step [700/938], Loss: 0.2809\n",
            "Epoch [4/10], Step [800/938], Loss: 0.3545\n",
            "Epoch [4/10], Step [900/938], Loss: 0.3671\n",
            "Epoch [5/10], Step [100/938], Loss: 0.2534\n",
            "Epoch [5/10], Step [200/938], Loss: 0.3161\n",
            "Epoch [5/10], Step [300/938], Loss: 0.4165\n",
            "Epoch [5/10], Step [400/938], Loss: 0.2161\n",
            "Epoch [5/10], Step [500/938], Loss: 0.2640\n",
            "Epoch [5/10], Step [600/938], Loss: 0.2221\n",
            "Epoch [5/10], Step [700/938], Loss: 0.1882\n",
            "Epoch [5/10], Step [800/938], Loss: 0.2141\n",
            "Epoch [5/10], Step [900/938], Loss: 0.2154\n",
            "Epoch [6/10], Step [100/938], Loss: 0.1757\n",
            "Epoch [6/10], Step [200/938], Loss: 0.3278\n",
            "Epoch [6/10], Step [300/938], Loss: 0.2940\n",
            "Epoch [6/10], Step [400/938], Loss: 0.2702\n",
            "Epoch [6/10], Step [500/938], Loss: 0.2249\n",
            "Epoch [6/10], Step [600/938], Loss: 0.2274\n",
            "Epoch [6/10], Step [700/938], Loss: 0.4209\n",
            "Epoch [6/10], Step [800/938], Loss: 0.2578\n",
            "Epoch [6/10], Step [900/938], Loss: 0.1307\n",
            "Epoch [7/10], Step [100/938], Loss: 0.3437\n",
            "Epoch [7/10], Step [200/938], Loss: 0.2068\n",
            "Epoch [7/10], Step [300/938], Loss: 0.4485\n",
            "Epoch [7/10], Step [400/938], Loss: 0.2041\n",
            "Epoch [7/10], Step [500/938], Loss: 0.2499\n",
            "Epoch [7/10], Step [600/938], Loss: 0.2921\n",
            "Epoch [7/10], Step [700/938], Loss: 0.2583\n",
            "Epoch [7/10], Step [800/938], Loss: 0.3151\n",
            "Epoch [7/10], Step [900/938], Loss: 0.2151\n",
            "Epoch [8/10], Step [100/938], Loss: 0.1750\n",
            "Epoch [8/10], Step [200/938], Loss: 0.3985\n",
            "Epoch [8/10], Step [300/938], Loss: 0.2867\n",
            "Epoch [8/10], Step [400/938], Loss: 0.2321\n",
            "Epoch [8/10], Step [500/938], Loss: 0.2900\n",
            "Epoch [8/10], Step [600/938], Loss: 0.3379\n",
            "Epoch [8/10], Step [700/938], Loss: 0.2449\n",
            "Epoch [8/10], Step [800/938], Loss: 0.1373\n",
            "Epoch [8/10], Step [900/938], Loss: 0.2205\n",
            "Epoch [9/10], Step [100/938], Loss: 0.1652\n",
            "Epoch [9/10], Step [200/938], Loss: 0.2295\n",
            "Epoch [9/10], Step [300/938], Loss: 0.1288\n",
            "Epoch [9/10], Step [400/938], Loss: 0.2175\n",
            "Epoch [9/10], Step [500/938], Loss: 0.1704\n",
            "Epoch [9/10], Step [600/938], Loss: 0.0706\n",
            "Epoch [9/10], Step [700/938], Loss: 0.3194\n",
            "Epoch [9/10], Step [800/938], Loss: 0.1064\n",
            "Epoch [9/10], Step [900/938], Loss: 0.1452\n",
            "Epoch [10/10], Step [100/938], Loss: 0.2346\n",
            "Epoch [10/10], Step [200/938], Loss: 0.1165\n",
            "Epoch [10/10], Step [300/938], Loss: 0.1496\n",
            "Epoch [10/10], Step [400/938], Loss: 0.2522\n",
            "Epoch [10/10], Step [500/938], Loss: 0.0850\n",
            "Epoch [10/10], Step [600/938], Loss: 0.1636\n",
            "Epoch [10/10], Step [700/938], Loss: 0.1197\n",
            "Epoch [10/10], Step [800/938], Loss: 0.1698\n",
            "Epoch [10/10], Step [900/938], Loss: 0.2996\n",
            "Accuracy of the network on the 10000 test images: 93.53 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        images, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(dataloader_train), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in dataloader_test:\n",
        "        images, labels = data\n",
        "        outputs = net(images.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y43CkuUth_Xi",
        "outputId": "7e0cc831-cb2b-4346-9726-3410b0bb4c09"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [100/938], Loss: 2.1957\n",
            "Epoch [1/100], Step [200/938], Loss: 2.0055\n",
            "Epoch [1/100], Step [300/938], Loss: 1.7582\n",
            "Epoch [1/100], Step [400/938], Loss: 1.4326\n",
            "Epoch [1/100], Step [500/938], Loss: 1.0773\n",
            "Epoch [1/100], Step [600/938], Loss: 0.9920\n",
            "Epoch [1/100], Step [700/938], Loss: 0.7909\n",
            "Epoch [1/100], Step [800/938], Loss: 0.8345\n",
            "Epoch [1/100], Step [900/938], Loss: 0.8509\n",
            "Epoch [2/100], Step [100/938], Loss: 0.5742\n",
            "Epoch [2/100], Step [200/938], Loss: 0.6319\n",
            "Epoch [2/100], Step [300/938], Loss: 0.4407\n",
            "Epoch [2/100], Step [400/938], Loss: 0.5020\n",
            "Epoch [2/100], Step [500/938], Loss: 0.4623\n",
            "Epoch [2/100], Step [600/938], Loss: 0.4083\n",
            "Epoch [2/100], Step [700/938], Loss: 0.3688\n",
            "Epoch [2/100], Step [800/938], Loss: 0.3400\n",
            "Epoch [2/100], Step [900/938], Loss: 0.3990\n",
            "Epoch [3/100], Step [100/938], Loss: 0.2871\n",
            "Epoch [3/100], Step [200/938], Loss: 0.4463\n",
            "Epoch [3/100], Step [300/938], Loss: 0.3348\n",
            "Epoch [3/100], Step [400/938], Loss: 0.3458\n",
            "Epoch [3/100], Step [500/938], Loss: 0.2350\n",
            "Epoch [3/100], Step [600/938], Loss: 0.3271\n",
            "Epoch [3/100], Step [700/938], Loss: 0.3552\n",
            "Epoch [3/100], Step [800/938], Loss: 0.2710\n",
            "Epoch [3/100], Step [900/938], Loss: 0.3048\n",
            "Epoch [4/100], Step [100/938], Loss: 0.3194\n",
            "Epoch [4/100], Step [200/938], Loss: 0.2932\n",
            "Epoch [4/100], Step [300/938], Loss: 0.2352\n",
            "Epoch [4/100], Step [400/938], Loss: 0.1711\n",
            "Epoch [4/100], Step [500/938], Loss: 0.3196\n",
            "Epoch [4/100], Step [600/938], Loss: 0.4461\n",
            "Epoch [4/100], Step [700/938], Loss: 0.2407\n",
            "Epoch [4/100], Step [800/938], Loss: 0.2947\n",
            "Epoch [4/100], Step [900/938], Loss: 0.3534\n",
            "Epoch [5/100], Step [100/938], Loss: 0.3128\n",
            "Epoch [5/100], Step [200/938], Loss: 0.3294\n",
            "Epoch [5/100], Step [300/938], Loss: 0.1734\n",
            "Epoch [5/100], Step [400/938], Loss: 0.1705\n",
            "Epoch [5/100], Step [500/938], Loss: 0.2391\n",
            "Epoch [5/100], Step [600/938], Loss: 0.2333\n",
            "Epoch [5/100], Step [700/938], Loss: 0.3418\n",
            "Epoch [5/100], Step [800/938], Loss: 0.1982\n",
            "Epoch [5/100], Step [900/938], Loss: 0.3370\n",
            "Epoch [6/100], Step [100/938], Loss: 0.3708\n",
            "Epoch [6/100], Step [200/938], Loss: 0.2109\n",
            "Epoch [6/100], Step [300/938], Loss: 0.1081\n",
            "Epoch [6/100], Step [400/938], Loss: 0.1738\n",
            "Epoch [6/100], Step [500/938], Loss: 0.1846\n",
            "Epoch [6/100], Step [600/938], Loss: 0.2781\n",
            "Epoch [6/100], Step [700/938], Loss: 0.1524\n",
            "Epoch [6/100], Step [800/938], Loss: 0.1704\n",
            "Epoch [6/100], Step [900/938], Loss: 0.3109\n",
            "Epoch [7/100], Step [100/938], Loss: 0.2368\n",
            "Epoch [7/100], Step [200/938], Loss: 0.2343\n",
            "Epoch [7/100], Step [300/938], Loss: 0.1188\n",
            "Epoch [7/100], Step [400/938], Loss: 0.1198\n",
            "Epoch [7/100], Step [500/938], Loss: 0.2709\n",
            "Epoch [7/100], Step [600/938], Loss: 0.2683\n",
            "Epoch [7/100], Step [700/938], Loss: 0.3228\n",
            "Epoch [7/100], Step [800/938], Loss: 0.4461\n",
            "Epoch [7/100], Step [900/938], Loss: 0.2218\n",
            "Epoch [8/100], Step [100/938], Loss: 0.1456\n",
            "Epoch [8/100], Step [200/938], Loss: 0.0934\n",
            "Epoch [8/100], Step [300/938], Loss: 0.2660\n",
            "Epoch [8/100], Step [400/938], Loss: 0.2589\n",
            "Epoch [8/100], Step [500/938], Loss: 0.0541\n",
            "Epoch [8/100], Step [600/938], Loss: 0.1232\n",
            "Epoch [8/100], Step [700/938], Loss: 0.1187\n",
            "Epoch [8/100], Step [800/938], Loss: 0.3689\n",
            "Epoch [8/100], Step [900/938], Loss: 0.1797\n",
            "Epoch [9/100], Step [100/938], Loss: 0.2381\n",
            "Epoch [9/100], Step [200/938], Loss: 0.1202\n",
            "Epoch [9/100], Step [300/938], Loss: 0.3361\n",
            "Epoch [9/100], Step [400/938], Loss: 0.1944\n",
            "Epoch [9/100], Step [500/938], Loss: 0.1142\n",
            "Epoch [9/100], Step [600/938], Loss: 0.1623\n",
            "Epoch [9/100], Step [700/938], Loss: 0.2097\n",
            "Epoch [9/100], Step [800/938], Loss: 0.3081\n",
            "Epoch [9/100], Step [900/938], Loss: 0.1361\n",
            "Epoch [10/100], Step [100/938], Loss: 0.2648\n",
            "Epoch [10/100], Step [200/938], Loss: 0.1295\n",
            "Epoch [10/100], Step [300/938], Loss: 0.1196\n",
            "Epoch [10/100], Step [400/938], Loss: 0.2991\n",
            "Epoch [10/100], Step [500/938], Loss: 0.0948\n",
            "Epoch [10/100], Step [600/938], Loss: 0.1147\n",
            "Epoch [10/100], Step [700/938], Loss: 0.2576\n",
            "Epoch [10/100], Step [800/938], Loss: 0.1334\n",
            "Epoch [10/100], Step [900/938], Loss: 0.2418\n",
            "Epoch [11/100], Step [100/938], Loss: 0.1574\n",
            "Epoch [11/100], Step [200/938], Loss: 0.1609\n",
            "Epoch [11/100], Step [300/938], Loss: 0.1621\n",
            "Epoch [11/100], Step [400/938], Loss: 0.2840\n",
            "Epoch [11/100], Step [500/938], Loss: 0.2911\n",
            "Epoch [11/100], Step [600/938], Loss: 0.2149\n",
            "Epoch [11/100], Step [700/938], Loss: 0.1589\n",
            "Epoch [11/100], Step [800/938], Loss: 0.2017\n",
            "Epoch [11/100], Step [900/938], Loss: 0.1415\n",
            "Epoch [12/100], Step [100/938], Loss: 0.1623\n",
            "Epoch [12/100], Step [200/938], Loss: 0.0713\n",
            "Epoch [12/100], Step [300/938], Loss: 0.2131\n",
            "Epoch [12/100], Step [400/938], Loss: 0.1543\n",
            "Epoch [12/100], Step [500/938], Loss: 0.1175\n",
            "Epoch [12/100], Step [600/938], Loss: 0.0853\n",
            "Epoch [12/100], Step [700/938], Loss: 0.1976\n",
            "Epoch [12/100], Step [800/938], Loss: 0.2180\n",
            "Epoch [12/100], Step [900/938], Loss: 0.1647\n",
            "Epoch [13/100], Step [100/938], Loss: 0.1489\n",
            "Epoch [13/100], Step [200/938], Loss: 0.0980\n",
            "Epoch [13/100], Step [300/938], Loss: 0.1606\n",
            "Epoch [13/100], Step [400/938], Loss: 0.0938\n",
            "Epoch [13/100], Step [500/938], Loss: 0.1044\n",
            "Epoch [13/100], Step [600/938], Loss: 0.0729\n",
            "Epoch [13/100], Step [700/938], Loss: 0.0933\n",
            "Epoch [13/100], Step [800/938], Loss: 0.0793\n",
            "Epoch [13/100], Step [900/938], Loss: 0.1456\n",
            "Epoch [14/100], Step [100/938], Loss: 0.2909\n",
            "Epoch [14/100], Step [200/938], Loss: 0.1838\n",
            "Epoch [14/100], Step [300/938], Loss: 0.1671\n",
            "Epoch [14/100], Step [400/938], Loss: 0.4357\n",
            "Epoch [14/100], Step [500/938], Loss: 0.0808\n",
            "Epoch [14/100], Step [600/938], Loss: 0.1595\n",
            "Epoch [14/100], Step [700/938], Loss: 0.1551\n",
            "Epoch [14/100], Step [800/938], Loss: 0.1501\n",
            "Epoch [14/100], Step [900/938], Loss: 0.2062\n",
            "Epoch [15/100], Step [100/938], Loss: 0.1350\n",
            "Epoch [15/100], Step [200/938], Loss: 0.2458\n",
            "Epoch [15/100], Step [300/938], Loss: 0.0872\n",
            "Epoch [15/100], Step [400/938], Loss: 0.1266\n",
            "Epoch [15/100], Step [500/938], Loss: 0.0515\n",
            "Epoch [15/100], Step [600/938], Loss: 0.0845\n",
            "Epoch [15/100], Step [700/938], Loss: 0.1162\n",
            "Epoch [15/100], Step [800/938], Loss: 0.1652\n",
            "Epoch [15/100], Step [900/938], Loss: 0.1067\n",
            "Epoch [16/100], Step [100/938], Loss: 0.1194\n",
            "Epoch [16/100], Step [200/938], Loss: 0.0864\n",
            "Epoch [16/100], Step [300/938], Loss: 0.0600\n",
            "Epoch [16/100], Step [400/938], Loss: 0.0878\n",
            "Epoch [16/100], Step [500/938], Loss: 0.1024\n",
            "Epoch [16/100], Step [600/938], Loss: 0.2091\n",
            "Epoch [16/100], Step [700/938], Loss: 0.0696\n",
            "Epoch [16/100], Step [800/938], Loss: 0.1100\n",
            "Epoch [16/100], Step [900/938], Loss: 0.1048\n",
            "Epoch [17/100], Step [100/938], Loss: 0.1415\n",
            "Epoch [17/100], Step [200/938], Loss: 0.0407\n",
            "Epoch [17/100], Step [300/938], Loss: 0.0947\n",
            "Epoch [17/100], Step [400/938], Loss: 0.0808\n",
            "Epoch [17/100], Step [500/938], Loss: 0.2223\n",
            "Epoch [17/100], Step [600/938], Loss: 0.1442\n",
            "Epoch [17/100], Step [700/938], Loss: 0.3083\n",
            "Epoch [17/100], Step [800/938], Loss: 0.1445\n",
            "Epoch [17/100], Step [900/938], Loss: 0.2191\n",
            "Epoch [18/100], Step [100/938], Loss: 0.0671\n",
            "Epoch [18/100], Step [200/938], Loss: 0.2190\n",
            "Epoch [18/100], Step [300/938], Loss: 0.1718\n",
            "Epoch [18/100], Step [400/938], Loss: 0.1379\n",
            "Epoch [18/100], Step [500/938], Loss: 0.1197\n",
            "Epoch [18/100], Step [600/938], Loss: 0.1580\n",
            "Epoch [18/100], Step [700/938], Loss: 0.0545\n",
            "Epoch [18/100], Step [800/938], Loss: 0.0690\n",
            "Epoch [18/100], Step [900/938], Loss: 0.1368\n",
            "Epoch [19/100], Step [100/938], Loss: 0.0489\n",
            "Epoch [19/100], Step [200/938], Loss: 0.2570\n",
            "Epoch [19/100], Step [300/938], Loss: 0.1268\n",
            "Epoch [19/100], Step [400/938], Loss: 0.0818\n",
            "Epoch [19/100], Step [500/938], Loss: 0.0934\n",
            "Epoch [19/100], Step [600/938], Loss: 0.1459\n",
            "Epoch [19/100], Step [700/938], Loss: 0.0561\n",
            "Epoch [19/100], Step [800/938], Loss: 0.1010\n",
            "Epoch [19/100], Step [900/938], Loss: 0.2502\n",
            "Epoch [20/100], Step [100/938], Loss: 0.2081\n",
            "Epoch [20/100], Step [200/938], Loss: 0.2283\n",
            "Epoch [20/100], Step [300/938], Loss: 0.0321\n",
            "Epoch [20/100], Step [400/938], Loss: 0.1446\n",
            "Epoch [20/100], Step [500/938], Loss: 0.0888\n",
            "Epoch [20/100], Step [600/938], Loss: 0.2666\n",
            "Epoch [20/100], Step [700/938], Loss: 0.0678\n",
            "Epoch [20/100], Step [800/938], Loss: 0.0968\n",
            "Epoch [20/100], Step [900/938], Loss: 0.2333\n",
            "Epoch [21/100], Step [100/938], Loss: 0.1565\n",
            "Epoch [21/100], Step [200/938], Loss: 0.0725\n",
            "Epoch [21/100], Step [300/938], Loss: 0.0835\n",
            "Epoch [21/100], Step [400/938], Loss: 0.1791\n",
            "Epoch [21/100], Step [500/938], Loss: 0.0947\n",
            "Epoch [21/100], Step [600/938], Loss: 0.1174\n",
            "Epoch [21/100], Step [700/938], Loss: 0.1698\n",
            "Epoch [21/100], Step [800/938], Loss: 0.1827\n",
            "Epoch [21/100], Step [900/938], Loss: 0.0649\n",
            "Epoch [22/100], Step [100/938], Loss: 0.1137\n",
            "Epoch [22/100], Step [200/938], Loss: 0.1735\n",
            "Epoch [22/100], Step [300/938], Loss: 0.1919\n",
            "Epoch [22/100], Step [400/938], Loss: 0.1490\n",
            "Epoch [22/100], Step [500/938], Loss: 0.0878\n",
            "Epoch [22/100], Step [600/938], Loss: 0.1040\n",
            "Epoch [22/100], Step [700/938], Loss: 0.0600\n",
            "Epoch [22/100], Step [800/938], Loss: 0.1634\n",
            "Epoch [22/100], Step [900/938], Loss: 0.2093\n",
            "Epoch [23/100], Step [100/938], Loss: 0.2267\n",
            "Epoch [23/100], Step [200/938], Loss: 0.0598\n",
            "Epoch [23/100], Step [300/938], Loss: 0.1460\n",
            "Epoch [23/100], Step [400/938], Loss: 0.1617\n",
            "Epoch [23/100], Step [500/938], Loss: 0.0476\n",
            "Epoch [23/100], Step [600/938], Loss: 0.0612\n",
            "Epoch [23/100], Step [700/938], Loss: 0.0825\n",
            "Epoch [23/100], Step [800/938], Loss: 0.1415\n",
            "Epoch [23/100], Step [900/938], Loss: 0.0553\n",
            "Epoch [24/100], Step [100/938], Loss: 0.0584\n",
            "Epoch [24/100], Step [200/938], Loss: 0.1244\n",
            "Epoch [24/100], Step [300/938], Loss: 0.1702\n",
            "Epoch [24/100], Step [400/938], Loss: 0.1957\n",
            "Epoch [24/100], Step [500/938], Loss: 0.0403\n",
            "Epoch [24/100], Step [600/938], Loss: 0.0704\n",
            "Epoch [24/100], Step [700/938], Loss: 0.1869\n",
            "Epoch [24/100], Step [800/938], Loss: 0.0962\n",
            "Epoch [24/100], Step [900/938], Loss: 0.1461\n",
            "Epoch [25/100], Step [100/938], Loss: 0.1893\n",
            "Epoch [25/100], Step [200/938], Loss: 0.0666\n",
            "Epoch [25/100], Step [300/938], Loss: 0.0459\n",
            "Epoch [25/100], Step [400/938], Loss: 0.0529\n",
            "Epoch [25/100], Step [500/938], Loss: 0.0728\n",
            "Epoch [25/100], Step [600/938], Loss: 0.1120\n",
            "Epoch [25/100], Step [700/938], Loss: 0.1275\n",
            "Epoch [25/100], Step [800/938], Loss: 0.1045\n",
            "Epoch [25/100], Step [900/938], Loss: 0.1410\n",
            "Epoch [26/100], Step [100/938], Loss: 0.1772\n",
            "Epoch [26/100], Step [200/938], Loss: 0.0946\n",
            "Epoch [26/100], Step [300/938], Loss: 0.0709\n",
            "Epoch [26/100], Step [400/938], Loss: 0.0501\n",
            "Epoch [26/100], Step [500/938], Loss: 0.0703\n",
            "Epoch [26/100], Step [600/938], Loss: 0.0687\n",
            "Epoch [26/100], Step [700/938], Loss: 0.1232\n",
            "Epoch [26/100], Step [800/938], Loss: 0.1677\n",
            "Epoch [26/100], Step [900/938], Loss: 0.1260\n",
            "Epoch [27/100], Step [100/938], Loss: 0.2292\n",
            "Epoch [27/100], Step [200/938], Loss: 0.1836\n",
            "Epoch [27/100], Step [300/938], Loss: 0.0672\n",
            "Epoch [27/100], Step [400/938], Loss: 0.1859\n",
            "Epoch [27/100], Step [500/938], Loss: 0.0780\n",
            "Epoch [27/100], Step [600/938], Loss: 0.0727\n",
            "Epoch [27/100], Step [700/938], Loss: 0.1841\n",
            "Epoch [27/100], Step [800/938], Loss: 0.0429\n",
            "Epoch [27/100], Step [900/938], Loss: 0.2365\n",
            "Epoch [28/100], Step [100/938], Loss: 0.0588\n",
            "Epoch [28/100], Step [200/938], Loss: 0.0951\n",
            "Epoch [28/100], Step [300/938], Loss: 0.2207\n",
            "Epoch [28/100], Step [400/938], Loss: 0.0619\n",
            "Epoch [28/100], Step [500/938], Loss: 0.1454\n",
            "Epoch [28/100], Step [600/938], Loss: 0.0987\n",
            "Epoch [28/100], Step [700/938], Loss: 0.0653\n",
            "Epoch [28/100], Step [800/938], Loss: 0.0363\n",
            "Epoch [28/100], Step [900/938], Loss: 0.0408\n",
            "Epoch [29/100], Step [100/938], Loss: 0.2319\n",
            "Epoch [29/100], Step [200/938], Loss: 0.0520\n",
            "Epoch [29/100], Step [300/938], Loss: 0.0848\n",
            "Epoch [29/100], Step [400/938], Loss: 0.0383\n",
            "Epoch [29/100], Step [500/938], Loss: 0.0455\n",
            "Epoch [29/100], Step [600/938], Loss: 0.1845\n",
            "Epoch [29/100], Step [700/938], Loss: 0.0547\n",
            "Epoch [29/100], Step [800/938], Loss: 0.0404\n",
            "Epoch [29/100], Step [900/938], Loss: 0.0466\n",
            "Epoch [30/100], Step [100/938], Loss: 0.0506\n",
            "Epoch [30/100], Step [200/938], Loss: 0.1258\n",
            "Epoch [30/100], Step [300/938], Loss: 0.0240\n",
            "Epoch [30/100], Step [400/938], Loss: 0.1309\n",
            "Epoch [30/100], Step [500/938], Loss: 0.1249\n",
            "Epoch [30/100], Step [600/938], Loss: 0.0446\n",
            "Epoch [30/100], Step [700/938], Loss: 0.1664\n",
            "Epoch [30/100], Step [800/938], Loss: 0.1257\n",
            "Epoch [30/100], Step [900/938], Loss: 0.1164\n",
            "Epoch [31/100], Step [100/938], Loss: 0.0613\n",
            "Epoch [31/100], Step [200/938], Loss: 0.1632\n",
            "Epoch [31/100], Step [300/938], Loss: 0.0601\n",
            "Epoch [31/100], Step [400/938], Loss: 0.0861\n",
            "Epoch [31/100], Step [500/938], Loss: 0.0786\n",
            "Epoch [31/100], Step [600/938], Loss: 0.0582\n",
            "Epoch [31/100], Step [700/938], Loss: 0.0435\n",
            "Epoch [31/100], Step [800/938], Loss: 0.0653\n",
            "Epoch [31/100], Step [900/938], Loss: 0.1044\n",
            "Epoch [32/100], Step [100/938], Loss: 0.1114\n",
            "Epoch [32/100], Step [200/938], Loss: 0.0353\n",
            "Epoch [32/100], Step [300/938], Loss: 0.1317\n",
            "Epoch [32/100], Step [400/938], Loss: 0.0721\n",
            "Epoch [32/100], Step [500/938], Loss: 0.1422\n",
            "Epoch [32/100], Step [600/938], Loss: 0.0652\n",
            "Epoch [32/100], Step [700/938], Loss: 0.0833\n",
            "Epoch [32/100], Step [800/938], Loss: 0.0901\n",
            "Epoch [32/100], Step [900/938], Loss: 0.0542\n",
            "Epoch [33/100], Step [100/938], Loss: 0.0510\n",
            "Epoch [33/100], Step [200/938], Loss: 0.0619\n",
            "Epoch [33/100], Step [300/938], Loss: 0.0741\n",
            "Epoch [33/100], Step [400/938], Loss: 0.0851\n",
            "Epoch [33/100], Step [500/938], Loss: 0.1048\n",
            "Epoch [33/100], Step [600/938], Loss: 0.0654\n",
            "Epoch [33/100], Step [700/938], Loss: 0.1008\n",
            "Epoch [33/100], Step [800/938], Loss: 0.1558\n",
            "Epoch [33/100], Step [900/938], Loss: 0.1518\n",
            "Epoch [34/100], Step [100/938], Loss: 0.2107\n",
            "Epoch [34/100], Step [200/938], Loss: 0.0392\n",
            "Epoch [34/100], Step [300/938], Loss: 0.2030\n",
            "Epoch [34/100], Step [400/938], Loss: 0.1702\n",
            "Epoch [34/100], Step [500/938], Loss: 0.1191\n",
            "Epoch [34/100], Step [600/938], Loss: 0.0550\n",
            "Epoch [34/100], Step [700/938], Loss: 0.0891\n",
            "Epoch [34/100], Step [800/938], Loss: 0.0510\n",
            "Epoch [34/100], Step [900/938], Loss: 0.2544\n",
            "Epoch [35/100], Step [100/938], Loss: 0.1182\n",
            "Epoch [35/100], Step [200/938], Loss: 0.0220\n",
            "Epoch [35/100], Step [300/938], Loss: 0.1053\n",
            "Epoch [35/100], Step [400/938], Loss: 0.1433\n",
            "Epoch [35/100], Step [500/938], Loss: 0.1034\n",
            "Epoch [35/100], Step [600/938], Loss: 0.1303\n",
            "Epoch [35/100], Step [700/938], Loss: 0.0502\n",
            "Epoch [35/100], Step [800/938], Loss: 0.1422\n",
            "Epoch [35/100], Step [900/938], Loss: 0.2000\n",
            "Epoch [36/100], Step [100/938], Loss: 0.0365\n",
            "Epoch [36/100], Step [200/938], Loss: 0.0641\n",
            "Epoch [36/100], Step [300/938], Loss: 0.0307\n",
            "Epoch [36/100], Step [400/938], Loss: 0.1060\n",
            "Epoch [36/100], Step [500/938], Loss: 0.0679\n",
            "Epoch [36/100], Step [600/938], Loss: 0.1323\n",
            "Epoch [36/100], Step [700/938], Loss: 0.1071\n",
            "Epoch [36/100], Step [800/938], Loss: 0.0606\n",
            "Epoch [36/100], Step [900/938], Loss: 0.1327\n",
            "Epoch [37/100], Step [100/938], Loss: 0.0793\n",
            "Epoch [37/100], Step [200/938], Loss: 0.1151\n",
            "Epoch [37/100], Step [300/938], Loss: 0.1134\n",
            "Epoch [37/100], Step [400/938], Loss: 0.0712\n",
            "Epoch [37/100], Step [500/938], Loss: 0.0390\n",
            "Epoch [37/100], Step [600/938], Loss: 0.1442\n",
            "Epoch [37/100], Step [700/938], Loss: 0.0593\n",
            "Epoch [37/100], Step [800/938], Loss: 0.1902\n",
            "Epoch [37/100], Step [900/938], Loss: 0.0861\n",
            "Epoch [38/100], Step [100/938], Loss: 0.1140\n",
            "Epoch [38/100], Step [200/938], Loss: 0.1411\n",
            "Epoch [38/100], Step [300/938], Loss: 0.0444\n",
            "Epoch [38/100], Step [400/938], Loss: 0.1611\n",
            "Epoch [38/100], Step [500/938], Loss: 0.0859\n",
            "Epoch [38/100], Step [600/938], Loss: 0.1104\n",
            "Epoch [38/100], Step [700/938], Loss: 0.0379\n",
            "Epoch [38/100], Step [800/938], Loss: 0.0365\n",
            "Epoch [38/100], Step [900/938], Loss: 0.0827\n",
            "Epoch [39/100], Step [100/938], Loss: 0.1447\n",
            "Epoch [39/100], Step [200/938], Loss: 0.0945\n",
            "Epoch [39/100], Step [300/938], Loss: 0.0680\n",
            "Epoch [39/100], Step [400/938], Loss: 0.1123\n",
            "Epoch [39/100], Step [500/938], Loss: 0.0736\n",
            "Epoch [39/100], Step [600/938], Loss: 0.1527\n",
            "Epoch [39/100], Step [700/938], Loss: 0.0525\n",
            "Epoch [39/100], Step [800/938], Loss: 0.0298\n",
            "Epoch [39/100], Step [900/938], Loss: 0.0936\n",
            "Epoch [40/100], Step [100/938], Loss: 0.0372\n",
            "Epoch [40/100], Step [200/938], Loss: 0.1162\n",
            "Epoch [40/100], Step [300/938], Loss: 0.1509\n",
            "Epoch [40/100], Step [400/938], Loss: 0.0848\n",
            "Epoch [40/100], Step [500/938], Loss: 0.0452\n",
            "Epoch [40/100], Step [600/938], Loss: 0.1189\n",
            "Epoch [40/100], Step [700/938], Loss: 0.0930\n",
            "Epoch [40/100], Step [800/938], Loss: 0.0684\n",
            "Epoch [40/100], Step [900/938], Loss: 0.0664\n",
            "Epoch [41/100], Step [100/938], Loss: 0.1309\n",
            "Epoch [41/100], Step [200/938], Loss: 0.0484\n",
            "Epoch [41/100], Step [300/938], Loss: 0.1809\n",
            "Epoch [41/100], Step [400/938], Loss: 0.1004\n",
            "Epoch [41/100], Step [500/938], Loss: 0.2227\n",
            "Epoch [41/100], Step [600/938], Loss: 0.0916\n",
            "Epoch [41/100], Step [700/938], Loss: 0.0736\n",
            "Epoch [41/100], Step [800/938], Loss: 0.0656\n",
            "Epoch [41/100], Step [900/938], Loss: 0.1161\n",
            "Epoch [42/100], Step [100/938], Loss: 0.1808\n",
            "Epoch [42/100], Step [200/938], Loss: 0.0985\n",
            "Epoch [42/100], Step [300/938], Loss: 0.0274\n",
            "Epoch [42/100], Step [400/938], Loss: 0.0386\n",
            "Epoch [42/100], Step [500/938], Loss: 0.1602\n",
            "Epoch [42/100], Step [600/938], Loss: 0.1284\n",
            "Epoch [42/100], Step [700/938], Loss: 0.0480\n",
            "Epoch [42/100], Step [800/938], Loss: 0.1229\n",
            "Epoch [42/100], Step [900/938], Loss: 0.0615\n",
            "Epoch [43/100], Step [100/938], Loss: 0.1724\n",
            "Epoch [43/100], Step [200/938], Loss: 0.0857\n",
            "Epoch [43/100], Step [300/938], Loss: 0.1256\n",
            "Epoch [43/100], Step [400/938], Loss: 0.0240\n",
            "Epoch [43/100], Step [500/938], Loss: 0.0272\n",
            "Epoch [43/100], Step [600/938], Loss: 0.0473\n",
            "Epoch [43/100], Step [700/938], Loss: 0.0669\n",
            "Epoch [43/100], Step [800/938], Loss: 0.0422\n",
            "Epoch [43/100], Step [900/938], Loss: 0.1160\n",
            "Epoch [44/100], Step [100/938], Loss: 0.0727\n",
            "Epoch [44/100], Step [200/938], Loss: 0.0440\n",
            "Epoch [44/100], Step [300/938], Loss: 0.0526\n",
            "Epoch [44/100], Step [400/938], Loss: 0.0712\n",
            "Epoch [44/100], Step [500/938], Loss: 0.1460\n",
            "Epoch [44/100], Step [600/938], Loss: 0.1736\n",
            "Epoch [44/100], Step [700/938], Loss: 0.1198\n",
            "Epoch [44/100], Step [800/938], Loss: 0.1510\n",
            "Epoch [44/100], Step [900/938], Loss: 0.0778\n",
            "Epoch [45/100], Step [100/938], Loss: 0.0503\n",
            "Epoch [45/100], Step [200/938], Loss: 0.0758\n",
            "Epoch [45/100], Step [300/938], Loss: 0.1143\n",
            "Epoch [45/100], Step [400/938], Loss: 0.1699\n",
            "Epoch [45/100], Step [500/938], Loss: 0.0771\n",
            "Epoch [45/100], Step [600/938], Loss: 0.1050\n",
            "Epoch [45/100], Step [700/938], Loss: 0.0637\n",
            "Epoch [45/100], Step [800/938], Loss: 0.1537\n",
            "Epoch [45/100], Step [900/938], Loss: 0.0646\n",
            "Epoch [46/100], Step [100/938], Loss: 0.0792\n",
            "Epoch [46/100], Step [200/938], Loss: 0.1605\n",
            "Epoch [46/100], Step [300/938], Loss: 0.0875\n",
            "Epoch [46/100], Step [400/938], Loss: 0.1555\n",
            "Epoch [46/100], Step [500/938], Loss: 0.1388\n",
            "Epoch [46/100], Step [600/938], Loss: 0.0608\n",
            "Epoch [46/100], Step [700/938], Loss: 0.1071\n",
            "Epoch [46/100], Step [800/938], Loss: 0.0550\n",
            "Epoch [46/100], Step [900/938], Loss: 0.0263\n",
            "Epoch [47/100], Step [100/938], Loss: 0.0752\n",
            "Epoch [47/100], Step [200/938], Loss: 0.0641\n",
            "Epoch [47/100], Step [300/938], Loss: 0.0204\n",
            "Epoch [47/100], Step [400/938], Loss: 0.0898\n",
            "Epoch [47/100], Step [500/938], Loss: 0.0943\n",
            "Epoch [47/100], Step [600/938], Loss: 0.0794\n",
            "Epoch [47/100], Step [700/938], Loss: 0.1431\n",
            "Epoch [47/100], Step [800/938], Loss: 0.0364\n",
            "Epoch [47/100], Step [900/938], Loss: 0.1145\n",
            "Epoch [48/100], Step [100/938], Loss: 0.1362\n",
            "Epoch [48/100], Step [200/938], Loss: 0.1592\n",
            "Epoch [48/100], Step [300/938], Loss: 0.0428\n",
            "Epoch [48/100], Step [400/938], Loss: 0.1605\n",
            "Epoch [48/100], Step [500/938], Loss: 0.0653\n",
            "Epoch [48/100], Step [600/938], Loss: 0.0426\n",
            "Epoch [48/100], Step [700/938], Loss: 0.0780\n",
            "Epoch [48/100], Step [800/938], Loss: 0.0845\n",
            "Epoch [48/100], Step [900/938], Loss: 0.0729\n",
            "Epoch [49/100], Step [100/938], Loss: 0.0992\n",
            "Epoch [49/100], Step [200/938], Loss: 0.1117\n",
            "Epoch [49/100], Step [300/938], Loss: 0.0475\n",
            "Epoch [49/100], Step [400/938], Loss: 0.2100\n",
            "Epoch [49/100], Step [500/938], Loss: 0.0665\n",
            "Epoch [49/100], Step [600/938], Loss: 0.0898\n",
            "Epoch [49/100], Step [700/938], Loss: 0.2096\n",
            "Epoch [49/100], Step [800/938], Loss: 0.0867\n",
            "Epoch [49/100], Step [900/938], Loss: 0.0230\n",
            "Epoch [50/100], Step [100/938], Loss: 0.1016\n",
            "Epoch [50/100], Step [200/938], Loss: 0.0778\n",
            "Epoch [50/100], Step [300/938], Loss: 0.0940\n",
            "Epoch [50/100], Step [400/938], Loss: 0.0618\n",
            "Epoch [50/100], Step [500/938], Loss: 0.0872\n",
            "Epoch [50/100], Step [600/938], Loss: 0.0324\n",
            "Epoch [50/100], Step [700/938], Loss: 0.0210\n",
            "Epoch [50/100], Step [800/938], Loss: 0.0852\n",
            "Epoch [50/100], Step [900/938], Loss: 0.0345\n",
            "Epoch [51/100], Step [100/938], Loss: 0.1384\n",
            "Epoch [51/100], Step [200/938], Loss: 0.0497\n",
            "Epoch [51/100], Step [300/938], Loss: 0.0393\n",
            "Epoch [51/100], Step [400/938], Loss: 0.0422\n",
            "Epoch [51/100], Step [500/938], Loss: 0.1151\n",
            "Epoch [51/100], Step [600/938], Loss: 0.1041\n",
            "Epoch [51/100], Step [700/938], Loss: 0.0681\n",
            "Epoch [51/100], Step [800/938], Loss: 0.1356\n",
            "Epoch [51/100], Step [900/938], Loss: 0.0678\n",
            "Epoch [52/100], Step [100/938], Loss: 0.0533\n",
            "Epoch [52/100], Step [200/938], Loss: 0.0382\n",
            "Epoch [52/100], Step [300/938], Loss: 0.2163\n",
            "Epoch [52/100], Step [400/938], Loss: 0.0574\n",
            "Epoch [52/100], Step [500/938], Loss: 0.0841\n",
            "Epoch [52/100], Step [600/938], Loss: 0.0439\n",
            "Epoch [52/100], Step [700/938], Loss: 0.0276\n",
            "Epoch [52/100], Step [800/938], Loss: 0.0245\n",
            "Epoch [52/100], Step [900/938], Loss: 0.0422\n",
            "Epoch [53/100], Step [100/938], Loss: 0.0247\n",
            "Epoch [53/100], Step [200/938], Loss: 0.1102\n",
            "Epoch [53/100], Step [300/938], Loss: 0.1380\n",
            "Epoch [53/100], Step [400/938], Loss: 0.0670\n",
            "Epoch [53/100], Step [500/938], Loss: 0.1317\n",
            "Epoch [53/100], Step [600/938], Loss: 0.2447\n",
            "Epoch [53/100], Step [700/938], Loss: 0.0843\n",
            "Epoch [53/100], Step [800/938], Loss: 0.0651\n",
            "Epoch [53/100], Step [900/938], Loss: 0.0282\n",
            "Epoch [54/100], Step [100/938], Loss: 0.0809\n",
            "Epoch [54/100], Step [200/938], Loss: 0.0434\n",
            "Epoch [54/100], Step [300/938], Loss: 0.1066\n",
            "Epoch [54/100], Step [400/938], Loss: 0.0919\n",
            "Epoch [54/100], Step [500/938], Loss: 0.1337\n",
            "Epoch [54/100], Step [600/938], Loss: 0.0427\n",
            "Epoch [54/100], Step [700/938], Loss: 0.2102\n",
            "Epoch [54/100], Step [800/938], Loss: 0.1306\n",
            "Epoch [54/100], Step [900/938], Loss: 0.0888\n",
            "Epoch [55/100], Step [100/938], Loss: 0.0375\n",
            "Epoch [55/100], Step [200/938], Loss: 0.0355\n",
            "Epoch [55/100], Step [300/938], Loss: 0.0940\n",
            "Epoch [55/100], Step [400/938], Loss: 0.0771\n",
            "Epoch [55/100], Step [500/938], Loss: 0.1358\n",
            "Epoch [55/100], Step [600/938], Loss: 0.0725\n",
            "Epoch [55/100], Step [700/938], Loss: 0.1263\n",
            "Epoch [55/100], Step [800/938], Loss: 0.0718\n",
            "Epoch [55/100], Step [900/938], Loss: 0.0317\n",
            "Epoch [56/100], Step [100/938], Loss: 0.1106\n",
            "Epoch [56/100], Step [200/938], Loss: 0.0751\n",
            "Epoch [56/100], Step [300/938], Loss: 0.0789\n",
            "Epoch [56/100], Step [400/938], Loss: 0.0326\n",
            "Epoch [56/100], Step [500/938], Loss: 0.0603\n",
            "Epoch [56/100], Step [600/938], Loss: 0.1106\n",
            "Epoch [56/100], Step [700/938], Loss: 0.0586\n",
            "Epoch [56/100], Step [800/938], Loss: 0.0707\n",
            "Epoch [56/100], Step [900/938], Loss: 0.0898\n",
            "Epoch [57/100], Step [100/938], Loss: 0.1089\n",
            "Epoch [57/100], Step [200/938], Loss: 0.0768\n",
            "Epoch [57/100], Step [300/938], Loss: 0.0287\n",
            "Epoch [57/100], Step [400/938], Loss: 0.0230\n",
            "Epoch [57/100], Step [500/938], Loss: 0.0740\n",
            "Epoch [57/100], Step [600/938], Loss: 0.0610\n",
            "Epoch [57/100], Step [700/938], Loss: 0.0605\n",
            "Epoch [57/100], Step [800/938], Loss: 0.0527\n",
            "Epoch [57/100], Step [900/938], Loss: 0.0220\n",
            "Epoch [58/100], Step [100/938], Loss: 0.0768\n",
            "Epoch [58/100], Step [200/938], Loss: 0.0329\n",
            "Epoch [58/100], Step [300/938], Loss: 0.0366\n",
            "Epoch [58/100], Step [400/938], Loss: 0.1994\n",
            "Epoch [58/100], Step [500/938], Loss: 0.1902\n",
            "Epoch [58/100], Step [600/938], Loss: 0.0536\n",
            "Epoch [58/100], Step [700/938], Loss: 0.0994\n",
            "Epoch [58/100], Step [800/938], Loss: 0.0349\n",
            "Epoch [58/100], Step [900/938], Loss: 0.0843\n",
            "Epoch [59/100], Step [100/938], Loss: 0.0593\n",
            "Epoch [59/100], Step [200/938], Loss: 0.1250\n",
            "Epoch [59/100], Step [300/938], Loss: 0.0755\n",
            "Epoch [59/100], Step [400/938], Loss: 0.0613\n",
            "Epoch [59/100], Step [500/938], Loss: 0.0148\n",
            "Epoch [59/100], Step [600/938], Loss: 0.0579\n",
            "Epoch [59/100], Step [700/938], Loss: 0.0289\n",
            "Epoch [59/100], Step [800/938], Loss: 0.0484\n",
            "Epoch [59/100], Step [900/938], Loss: 0.1106\n",
            "Epoch [60/100], Step [100/938], Loss: 0.0810\n",
            "Epoch [60/100], Step [200/938], Loss: 0.0992\n",
            "Epoch [60/100], Step [300/938], Loss: 0.0515\n",
            "Epoch [60/100], Step [400/938], Loss: 0.0559\n",
            "Epoch [60/100], Step [500/938], Loss: 0.0731\n",
            "Epoch [60/100], Step [600/938], Loss: 0.1489\n",
            "Epoch [60/100], Step [700/938], Loss: 0.1347\n",
            "Epoch [60/100], Step [800/938], Loss: 0.0253\n",
            "Epoch [60/100], Step [900/938], Loss: 0.0294\n",
            "Epoch [61/100], Step [100/938], Loss: 0.0712\n",
            "Epoch [61/100], Step [200/938], Loss: 0.0688\n",
            "Epoch [61/100], Step [300/938], Loss: 0.1145\n",
            "Epoch [61/100], Step [400/938], Loss: 0.0589\n",
            "Epoch [61/100], Step [500/938], Loss: 0.0635\n",
            "Epoch [61/100], Step [600/938], Loss: 0.0884\n",
            "Epoch [61/100], Step [700/938], Loss: 0.0091\n",
            "Epoch [61/100], Step [800/938], Loss: 0.1073\n",
            "Epoch [61/100], Step [900/938], Loss: 0.1599\n",
            "Epoch [62/100], Step [100/938], Loss: 0.0890\n",
            "Epoch [62/100], Step [200/938], Loss: 0.1169\n",
            "Epoch [62/100], Step [300/938], Loss: 0.0209\n",
            "Epoch [62/100], Step [400/938], Loss: 0.0170\n",
            "Epoch [62/100], Step [500/938], Loss: 0.0606\n",
            "Epoch [62/100], Step [600/938], Loss: 0.0386\n",
            "Epoch [62/100], Step [700/938], Loss: 0.0981\n",
            "Epoch [62/100], Step [800/938], Loss: 0.1513\n",
            "Epoch [62/100], Step [900/938], Loss: 0.0532\n",
            "Epoch [63/100], Step [100/938], Loss: 0.0764\n",
            "Epoch [63/100], Step [200/938], Loss: 0.0289\n",
            "Epoch [63/100], Step [300/938], Loss: 0.0182\n",
            "Epoch [63/100], Step [400/938], Loss: 0.0954\n",
            "Epoch [63/100], Step [500/938], Loss: 0.0552\n",
            "Epoch [63/100], Step [600/938], Loss: 0.0755\n",
            "Epoch [63/100], Step [700/938], Loss: 0.0342\n",
            "Epoch [63/100], Step [800/938], Loss: 0.0152\n",
            "Epoch [63/100], Step [900/938], Loss: 0.1053\n",
            "Epoch [64/100], Step [100/938], Loss: 0.1534\n",
            "Epoch [64/100], Step [200/938], Loss: 0.0389\n",
            "Epoch [64/100], Step [300/938], Loss: 0.0089\n",
            "Epoch [64/100], Step [400/938], Loss: 0.1314\n",
            "Epoch [64/100], Step [500/938], Loss: 0.0573\n",
            "Epoch [64/100], Step [600/938], Loss: 0.0565\n",
            "Epoch [64/100], Step [700/938], Loss: 0.0853\n",
            "Epoch [64/100], Step [800/938], Loss: 0.0785\n",
            "Epoch [64/100], Step [900/938], Loss: 0.0970\n",
            "Epoch [65/100], Step [100/938], Loss: 0.0483\n",
            "Epoch [65/100], Step [200/938], Loss: 0.0385\n",
            "Epoch [65/100], Step [300/938], Loss: 0.0595\n",
            "Epoch [65/100], Step [400/938], Loss: 0.0877\n",
            "Epoch [65/100], Step [500/938], Loss: 0.0199\n",
            "Epoch [65/100], Step [600/938], Loss: 0.0430\n",
            "Epoch [65/100], Step [700/938], Loss: 0.0780\n",
            "Epoch [65/100], Step [800/938], Loss: 0.0252\n",
            "Epoch [65/100], Step [900/938], Loss: 0.0469\n",
            "Epoch [66/100], Step [100/938], Loss: 0.0233\n",
            "Epoch [66/100], Step [200/938], Loss: 0.1761\n",
            "Epoch [66/100], Step [300/938], Loss: 0.0809\n",
            "Epoch [66/100], Step [400/938], Loss: 0.1927\n",
            "Epoch [66/100], Step [500/938], Loss: 0.0787\n",
            "Epoch [66/100], Step [600/938], Loss: 0.0823\n",
            "Epoch [66/100], Step [700/938], Loss: 0.1011\n",
            "Epoch [66/100], Step [800/938], Loss: 0.0773\n",
            "Epoch [66/100], Step [900/938], Loss: 0.0579\n",
            "Epoch [67/100], Step [100/938], Loss: 0.0783\n",
            "Epoch [67/100], Step [200/938], Loss: 0.0982\n",
            "Epoch [67/100], Step [300/938], Loss: 0.1255\n",
            "Epoch [67/100], Step [400/938], Loss: 0.0857\n",
            "Epoch [67/100], Step [500/938], Loss: 0.1006\n",
            "Epoch [67/100], Step [600/938], Loss: 0.0267\n",
            "Epoch [67/100], Step [700/938], Loss: 0.0602\n",
            "Epoch [67/100], Step [800/938], Loss: 0.0700\n",
            "Epoch [67/100], Step [900/938], Loss: 0.1248\n",
            "Epoch [68/100], Step [100/938], Loss: 0.0467\n",
            "Epoch [68/100], Step [200/938], Loss: 0.0498\n",
            "Epoch [68/100], Step [300/938], Loss: 0.0679\n",
            "Epoch [68/100], Step [400/938], Loss: 0.0292\n",
            "Epoch [68/100], Step [500/938], Loss: 0.0897\n",
            "Epoch [68/100], Step [600/938], Loss: 0.1023\n",
            "Epoch [68/100], Step [700/938], Loss: 0.0778\n",
            "Epoch [68/100], Step [800/938], Loss: 0.1289\n",
            "Epoch [68/100], Step [900/938], Loss: 0.0246\n",
            "Epoch [69/100], Step [100/938], Loss: 0.0332\n",
            "Epoch [69/100], Step [200/938], Loss: 0.0352\n",
            "Epoch [69/100], Step [300/938], Loss: 0.0365\n",
            "Epoch [69/100], Step [400/938], Loss: 0.0171\n",
            "Epoch [69/100], Step [500/938], Loss: 0.0940\n",
            "Epoch [69/100], Step [600/938], Loss: 0.0956\n",
            "Epoch [69/100], Step [700/938], Loss: 0.0769\n",
            "Epoch [69/100], Step [800/938], Loss: 0.0831\n",
            "Epoch [69/100], Step [900/938], Loss: 0.0628\n",
            "Epoch [70/100], Step [100/938], Loss: 0.0207\n",
            "Epoch [70/100], Step [200/938], Loss: 0.0971\n",
            "Epoch [70/100], Step [300/938], Loss: 0.0263\n",
            "Epoch [70/100], Step [400/938], Loss: 0.0243\n",
            "Epoch [70/100], Step [500/938], Loss: 0.0548\n",
            "Epoch [70/100], Step [600/938], Loss: 0.0588\n",
            "Epoch [70/100], Step [700/938], Loss: 0.1353\n",
            "Epoch [70/100], Step [800/938], Loss: 0.0095\n",
            "Epoch [70/100], Step [900/938], Loss: 0.0413\n",
            "Epoch [71/100], Step [100/938], Loss: 0.1399\n",
            "Epoch [71/100], Step [200/938], Loss: 0.0818\n",
            "Epoch [71/100], Step [300/938], Loss: 0.0405\n",
            "Epoch [71/100], Step [400/938], Loss: 0.0460\n",
            "Epoch [71/100], Step [500/938], Loss: 0.0779\n",
            "Epoch [71/100], Step [600/938], Loss: 0.0635\n",
            "Epoch [71/100], Step [700/938], Loss: 0.1647\n",
            "Epoch [71/100], Step [800/938], Loss: 0.1951\n",
            "Epoch [71/100], Step [900/938], Loss: 0.0681\n",
            "Epoch [72/100], Step [100/938], Loss: 0.0874\n",
            "Epoch [72/100], Step [200/938], Loss: 0.0611\n",
            "Epoch [72/100], Step [300/938], Loss: 0.0154\n",
            "Epoch [72/100], Step [400/938], Loss: 0.0572\n",
            "Epoch [72/100], Step [500/938], Loss: 0.0259\n",
            "Epoch [72/100], Step [600/938], Loss: 0.1337\n",
            "Epoch [72/100], Step [700/938], Loss: 0.0456\n",
            "Epoch [72/100], Step [800/938], Loss: 0.0419\n",
            "Epoch [72/100], Step [900/938], Loss: 0.0161\n",
            "Epoch [73/100], Step [100/938], Loss: 0.0133\n",
            "Epoch [73/100], Step [200/938], Loss: 0.0743\n",
            "Epoch [73/100], Step [300/938], Loss: 0.0194\n",
            "Epoch [73/100], Step [400/938], Loss: 0.1507\n",
            "Epoch [73/100], Step [500/938], Loss: 0.0336\n",
            "Epoch [73/100], Step [600/938], Loss: 0.1866\n",
            "Epoch [73/100], Step [700/938], Loss: 0.0689\n",
            "Epoch [73/100], Step [800/938], Loss: 0.1262\n",
            "Epoch [73/100], Step [900/938], Loss: 0.0891\n",
            "Epoch [74/100], Step [100/938], Loss: 0.1200\n",
            "Epoch [74/100], Step [200/938], Loss: 0.0084\n",
            "Epoch [74/100], Step [300/938], Loss: 0.0837\n",
            "Epoch [74/100], Step [400/938], Loss: 0.0304\n",
            "Epoch [74/100], Step [500/938], Loss: 0.0615\n",
            "Epoch [74/100], Step [600/938], Loss: 0.1096\n",
            "Epoch [74/100], Step [700/938], Loss: 0.0143\n",
            "Epoch [74/100], Step [800/938], Loss: 0.0232\n",
            "Epoch [74/100], Step [900/938], Loss: 0.0135\n",
            "Epoch [75/100], Step [100/938], Loss: 0.0520\n",
            "Epoch [75/100], Step [200/938], Loss: 0.0293\n",
            "Epoch [75/100], Step [300/938], Loss: 0.0666\n",
            "Epoch [75/100], Step [400/938], Loss: 0.0333\n",
            "Epoch [75/100], Step [500/938], Loss: 0.0516\n",
            "Epoch [75/100], Step [600/938], Loss: 0.0868\n",
            "Epoch [75/100], Step [700/938], Loss: 0.0404\n",
            "Epoch [75/100], Step [800/938], Loss: 0.0731\n",
            "Epoch [75/100], Step [900/938], Loss: 0.0791\n",
            "Epoch [76/100], Step [100/938], Loss: 0.0518\n",
            "Epoch [76/100], Step [200/938], Loss: 0.0365\n",
            "Epoch [76/100], Step [300/938], Loss: 0.0286\n",
            "Epoch [76/100], Step [400/938], Loss: 0.1581\n",
            "Epoch [76/100], Step [500/938], Loss: 0.0458\n",
            "Epoch [76/100], Step [600/938], Loss: 0.0596\n",
            "Epoch [76/100], Step [700/938], Loss: 0.0350\n",
            "Epoch [76/100], Step [800/938], Loss: 0.0461\n",
            "Epoch [76/100], Step [900/938], Loss: 0.2061\n",
            "Epoch [77/100], Step [100/938], Loss: 0.0133\n",
            "Epoch [77/100], Step [200/938], Loss: 0.0832\n",
            "Epoch [77/100], Step [300/938], Loss: 0.0743\n",
            "Epoch [77/100], Step [400/938], Loss: 0.1168\n",
            "Epoch [77/100], Step [500/938], Loss: 0.0334\n",
            "Epoch [77/100], Step [600/938], Loss: 0.0131\n",
            "Epoch [77/100], Step [700/938], Loss: 0.1099\n",
            "Epoch [77/100], Step [800/938], Loss: 0.0486\n",
            "Epoch [77/100], Step [900/938], Loss: 0.0656\n",
            "Epoch [78/100], Step [100/938], Loss: 0.0737\n",
            "Epoch [78/100], Step [200/938], Loss: 0.0418\n",
            "Epoch [78/100], Step [300/938], Loss: 0.0328\n",
            "Epoch [78/100], Step [400/938], Loss: 0.0245\n",
            "Epoch [78/100], Step [500/938], Loss: 0.1750\n",
            "Epoch [78/100], Step [600/938], Loss: 0.0595\n",
            "Epoch [78/100], Step [700/938], Loss: 0.2535\n",
            "Epoch [78/100], Step [800/938], Loss: 0.0267\n",
            "Epoch [78/100], Step [900/938], Loss: 0.0890\n",
            "Epoch [79/100], Step [100/938], Loss: 0.1230\n",
            "Epoch [79/100], Step [200/938], Loss: 0.0400\n",
            "Epoch [79/100], Step [300/938], Loss: 0.0893\n",
            "Epoch [79/100], Step [400/938], Loss: 0.1236\n",
            "Epoch [79/100], Step [500/938], Loss: 0.0380\n",
            "Epoch [79/100], Step [600/938], Loss: 0.1033\n",
            "Epoch [79/100], Step [700/938], Loss: 0.0205\n",
            "Epoch [79/100], Step [800/938], Loss: 0.1580\n",
            "Epoch [79/100], Step [900/938], Loss: 0.0084\n",
            "Epoch [80/100], Step [100/938], Loss: 0.0685\n",
            "Epoch [80/100], Step [200/938], Loss: 0.1044\n",
            "Epoch [80/100], Step [300/938], Loss: 0.0515\n",
            "Epoch [80/100], Step [400/938], Loss: 0.0166\n",
            "Epoch [80/100], Step [500/938], Loss: 0.0436\n",
            "Epoch [80/100], Step [600/938], Loss: 0.0253\n",
            "Epoch [80/100], Step [700/938], Loss: 0.0408\n",
            "Epoch [80/100], Step [800/938], Loss: 0.0371\n",
            "Epoch [80/100], Step [900/938], Loss: 0.0212\n",
            "Epoch [81/100], Step [100/938], Loss: 0.0873\n",
            "Epoch [81/100], Step [200/938], Loss: 0.0236\n",
            "Epoch [81/100], Step [300/938], Loss: 0.1009\n",
            "Epoch [81/100], Step [400/938], Loss: 0.0960\n",
            "Epoch [81/100], Step [500/938], Loss: 0.0931\n",
            "Epoch [81/100], Step [600/938], Loss: 0.0844\n",
            "Epoch [81/100], Step [700/938], Loss: 0.0128\n",
            "Epoch [81/100], Step [800/938], Loss: 0.0276\n",
            "Epoch [81/100], Step [900/938], Loss: 0.0527\n",
            "Epoch [82/100], Step [100/938], Loss: 0.0275\n",
            "Epoch [82/100], Step [200/938], Loss: 0.0321\n",
            "Epoch [82/100], Step [300/938], Loss: 0.0185\n",
            "Epoch [82/100], Step [400/938], Loss: 0.1088\n",
            "Epoch [82/100], Step [500/938], Loss: 0.0489\n",
            "Epoch [82/100], Step [600/938], Loss: 0.0959\n",
            "Epoch [82/100], Step [700/938], Loss: 0.0292\n",
            "Epoch [82/100], Step [800/938], Loss: 0.1647\n",
            "Epoch [82/100], Step [900/938], Loss: 0.0657\n",
            "Epoch [83/100], Step [100/938], Loss: 0.0205\n",
            "Epoch [83/100], Step [200/938], Loss: 0.0504\n",
            "Epoch [83/100], Step [300/938], Loss: 0.0445\n",
            "Epoch [83/100], Step [400/938], Loss: 0.0438\n",
            "Epoch [83/100], Step [500/938], Loss: 0.0130\n",
            "Epoch [83/100], Step [600/938], Loss: 0.0502\n",
            "Epoch [83/100], Step [700/938], Loss: 0.0403\n",
            "Epoch [83/100], Step [800/938], Loss: 0.0275\n",
            "Epoch [83/100], Step [900/938], Loss: 0.0894\n",
            "Epoch [84/100], Step [100/938], Loss: 0.0270\n",
            "Epoch [84/100], Step [200/938], Loss: 0.1060\n",
            "Epoch [84/100], Step [300/938], Loss: 0.0266\n",
            "Epoch [84/100], Step [400/938], Loss: 0.2199\n",
            "Epoch [84/100], Step [500/938], Loss: 0.1982\n",
            "Epoch [84/100], Step [600/938], Loss: 0.0428\n",
            "Epoch [84/100], Step [700/938], Loss: 0.0303\n",
            "Epoch [84/100], Step [800/938], Loss: 0.0153\n",
            "Epoch [84/100], Step [900/938], Loss: 0.0150\n",
            "Epoch [85/100], Step [100/938], Loss: 0.0115\n",
            "Epoch [85/100], Step [200/938], Loss: 0.0924\n",
            "Epoch [85/100], Step [300/938], Loss: 0.1508\n",
            "Epoch [85/100], Step [400/938], Loss: 0.0268\n",
            "Epoch [85/100], Step [500/938], Loss: 0.0824\n",
            "Epoch [85/100], Step [600/938], Loss: 0.0409\n",
            "Epoch [85/100], Step [700/938], Loss: 0.0593\n",
            "Epoch [85/100], Step [800/938], Loss: 0.0239\n",
            "Epoch [85/100], Step [900/938], Loss: 0.0651\n",
            "Epoch [86/100], Step [100/938], Loss: 0.0534\n",
            "Epoch [86/100], Step [200/938], Loss: 0.0633\n",
            "Epoch [86/100], Step [300/938], Loss: 0.0578\n",
            "Epoch [86/100], Step [400/938], Loss: 0.0232\n",
            "Epoch [86/100], Step [500/938], Loss: 0.1224\n",
            "Epoch [86/100], Step [600/938], Loss: 0.0313\n",
            "Epoch [86/100], Step [700/938], Loss: 0.0370\n",
            "Epoch [86/100], Step [800/938], Loss: 0.0627\n",
            "Epoch [86/100], Step [900/938], Loss: 0.0587\n",
            "Epoch [87/100], Step [100/938], Loss: 0.0368\n",
            "Epoch [87/100], Step [200/938], Loss: 0.0230\n",
            "Epoch [87/100], Step [300/938], Loss: 0.0539\n",
            "Epoch [87/100], Step [400/938], Loss: 0.0106\n",
            "Epoch [87/100], Step [500/938], Loss: 0.0438\n",
            "Epoch [87/100], Step [600/938], Loss: 0.1182\n",
            "Epoch [87/100], Step [700/938], Loss: 0.0450\n",
            "Epoch [87/100], Step [800/938], Loss: 0.0869\n",
            "Epoch [87/100], Step [900/938], Loss: 0.0356\n",
            "Epoch [88/100], Step [100/938], Loss: 0.0490\n",
            "Epoch [88/100], Step [200/938], Loss: 0.0119\n",
            "Epoch [88/100], Step [300/938], Loss: 0.1137\n",
            "Epoch [88/100], Step [400/938], Loss: 0.0260\n",
            "Epoch [88/100], Step [500/938], Loss: 0.0679\n",
            "Epoch [88/100], Step [600/938], Loss: 0.1230\n",
            "Epoch [88/100], Step [700/938], Loss: 0.0205\n",
            "Epoch [88/100], Step [800/938], Loss: 0.0531\n",
            "Epoch [88/100], Step [900/938], Loss: 0.0779\n",
            "Epoch [89/100], Step [100/938], Loss: 0.0302\n",
            "Epoch [89/100], Step [200/938], Loss: 0.0790\n",
            "Epoch [89/100], Step [300/938], Loss: 0.0524\n",
            "Epoch [89/100], Step [400/938], Loss: 0.0481\n",
            "Epoch [89/100], Step [500/938], Loss: 0.0480\n",
            "Epoch [89/100], Step [600/938], Loss: 0.0800\n",
            "Epoch [89/100], Step [700/938], Loss: 0.0354\n",
            "Epoch [89/100], Step [800/938], Loss: 0.0410\n",
            "Epoch [89/100], Step [900/938], Loss: 0.0313\n",
            "Epoch [90/100], Step [100/938], Loss: 0.0495\n",
            "Epoch [90/100], Step [200/938], Loss: 0.0444\n",
            "Epoch [90/100], Step [300/938], Loss: 0.0320\n",
            "Epoch [90/100], Step [400/938], Loss: 0.0400\n",
            "Epoch [90/100], Step [500/938], Loss: 0.0370\n",
            "Epoch [90/100], Step [600/938], Loss: 0.0318\n",
            "Epoch [90/100], Step [700/938], Loss: 0.0698\n",
            "Epoch [90/100], Step [800/938], Loss: 0.0985\n",
            "Epoch [90/100], Step [900/938], Loss: 0.0656\n",
            "Epoch [91/100], Step [100/938], Loss: 0.0849\n",
            "Epoch [91/100], Step [200/938], Loss: 0.0631\n",
            "Epoch [91/100], Step [300/938], Loss: 0.0305\n",
            "Epoch [91/100], Step [400/938], Loss: 0.0204\n",
            "Epoch [91/100], Step [500/938], Loss: 0.0426\n",
            "Epoch [91/100], Step [600/938], Loss: 0.0465\n",
            "Epoch [91/100], Step [700/938], Loss: 0.0337\n",
            "Epoch [91/100], Step [800/938], Loss: 0.0203\n",
            "Epoch [91/100], Step [900/938], Loss: 0.0405\n",
            "Epoch [92/100], Step [100/938], Loss: 0.0268\n",
            "Epoch [92/100], Step [200/938], Loss: 0.0614\n",
            "Epoch [92/100], Step [300/938], Loss: 0.2083\n",
            "Epoch [92/100], Step [400/938], Loss: 0.1094\n",
            "Epoch [92/100], Step [500/938], Loss: 0.0592\n",
            "Epoch [92/100], Step [600/938], Loss: 0.0287\n",
            "Epoch [92/100], Step [700/938], Loss: 0.0441\n",
            "Epoch [92/100], Step [800/938], Loss: 0.0203\n",
            "Epoch [92/100], Step [900/938], Loss: 0.0193\n",
            "Epoch [93/100], Step [100/938], Loss: 0.0302\n",
            "Epoch [93/100], Step [200/938], Loss: 0.0346\n",
            "Epoch [93/100], Step [300/938], Loss: 0.0640\n",
            "Epoch [93/100], Step [400/938], Loss: 0.0357\n",
            "Epoch [93/100], Step [500/938], Loss: 0.0348\n",
            "Epoch [93/100], Step [600/938], Loss: 0.0498\n",
            "Epoch [93/100], Step [700/938], Loss: 0.0626\n",
            "Epoch [93/100], Step [800/938], Loss: 0.0528\n",
            "Epoch [93/100], Step [900/938], Loss: 0.0321\n",
            "Epoch [94/100], Step [100/938], Loss: 0.0572\n",
            "Epoch [94/100], Step [200/938], Loss: 0.0851\n",
            "Epoch [94/100], Step [300/938], Loss: 0.0901\n",
            "Epoch [94/100], Step [400/938], Loss: 0.0196\n",
            "Epoch [94/100], Step [500/938], Loss: 0.0282\n",
            "Epoch [94/100], Step [600/938], Loss: 0.0668\n",
            "Epoch [94/100], Step [700/938], Loss: 0.0210\n",
            "Epoch [94/100], Step [800/938], Loss: 0.0634\n",
            "Epoch [94/100], Step [900/938], Loss: 0.0647\n",
            "Epoch [95/100], Step [100/938], Loss: 0.0251\n",
            "Epoch [95/100], Step [200/938], Loss: 0.0337\n",
            "Epoch [95/100], Step [300/938], Loss: 0.0447\n",
            "Epoch [95/100], Step [400/938], Loss: 0.0334\n",
            "Epoch [95/100], Step [500/938], Loss: 0.0592\n",
            "Epoch [95/100], Step [600/938], Loss: 0.0210\n",
            "Epoch [95/100], Step [700/938], Loss: 0.0488\n",
            "Epoch [95/100], Step [800/938], Loss: 0.0530\n",
            "Epoch [95/100], Step [900/938], Loss: 0.0424\n",
            "Epoch [96/100], Step [100/938], Loss: 0.0644\n",
            "Epoch [96/100], Step [200/938], Loss: 0.0282\n",
            "Epoch [96/100], Step [300/938], Loss: 0.0659\n",
            "Epoch [96/100], Step [400/938], Loss: 0.0280\n",
            "Epoch [96/100], Step [500/938], Loss: 0.0648\n",
            "Epoch [96/100], Step [600/938], Loss: 0.0297\n",
            "Epoch [96/100], Step [700/938], Loss: 0.1536\n",
            "Epoch [96/100], Step [800/938], Loss: 0.0886\n",
            "Epoch [96/100], Step [900/938], Loss: 0.0408\n",
            "Epoch [97/100], Step [100/938], Loss: 0.0166\n",
            "Epoch [97/100], Step [200/938], Loss: 0.0719\n",
            "Epoch [97/100], Step [300/938], Loss: 0.1875\n",
            "Epoch [97/100], Step [400/938], Loss: 0.0926\n",
            "Epoch [97/100], Step [500/938], Loss: 0.0551\n",
            "Epoch [97/100], Step [600/938], Loss: 0.0191\n",
            "Epoch [97/100], Step [700/938], Loss: 0.0141\n",
            "Epoch [97/100], Step [800/938], Loss: 0.0913\n",
            "Epoch [97/100], Step [900/938], Loss: 0.0864\n",
            "Epoch [98/100], Step [100/938], Loss: 0.0362\n",
            "Epoch [98/100], Step [200/938], Loss: 0.0566\n",
            "Epoch [98/100], Step [300/938], Loss: 0.0525\n",
            "Epoch [98/100], Step [400/938], Loss: 0.1281\n",
            "Epoch [98/100], Step [500/938], Loss: 0.0301\n",
            "Epoch [98/100], Step [600/938], Loss: 0.0867\n",
            "Epoch [98/100], Step [700/938], Loss: 0.0259\n",
            "Epoch [98/100], Step [800/938], Loss: 0.0209\n",
            "Epoch [98/100], Step [900/938], Loss: 0.0253\n",
            "Epoch [99/100], Step [100/938], Loss: 0.0286\n",
            "Epoch [99/100], Step [200/938], Loss: 0.0192\n",
            "Epoch [99/100], Step [300/938], Loss: 0.0612\n",
            "Epoch [99/100], Step [400/938], Loss: 0.0250\n",
            "Epoch [99/100], Step [500/938], Loss: 0.0271\n",
            "Epoch [99/100], Step [600/938], Loss: 0.0101\n",
            "Epoch [99/100], Step [700/938], Loss: 0.0364\n",
            "Epoch [99/100], Step [800/938], Loss: 0.0263\n",
            "Epoch [99/100], Step [900/938], Loss: 0.0226\n",
            "Epoch [100/100], Step [100/938], Loss: 0.1314\n",
            "Epoch [100/100], Step [200/938], Loss: 0.0560\n",
            "Epoch [100/100], Step [300/938], Loss: 0.0393\n",
            "Epoch [100/100], Step [400/938], Loss: 0.1871\n",
            "Epoch [100/100], Step [500/938], Loss: 0.0160\n",
            "Epoch [100/100], Step [600/938], Loss: 0.0535\n",
            "Epoch [100/100], Step [700/938], Loss: 0.1075\n",
            "Epoch [100/100], Step [800/938], Loss: 0.0735\n",
            "Epoch [100/100], Step [900/938], Loss: 0.0596\n",
            "Accuracy of the network on the 10000 test images: 96.98 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        h0 = torch.zeros(self.num_layers, self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, self.hidden_size)\n",
        "        # Passing in the input and hidden state into the model and  obtaining outputs\n",
        "        out, hidden = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        \n",
        "        #Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "       \n",
        "        pass\n",
        "pass\n",
        "model = RNN(20, 128, 2, 10)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Z3hKfNT73C",
        "outputId": "6d12f370-c332-45f8-98d5-4b53d703fa8e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (lstm): LSTM(20, 128, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and define the loss function and optimizer\n",
        "net = model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        images, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(dataloader_train), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in dataloader_test:\n",
        "        images, labels = data\n",
        "        outputs = net(images.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEb-jyjpOItB",
        "outputId": "ed0459ed-8aac-4fd7-c749-2ee3791aef5c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/938], Loss: 2.3097\n",
            "Epoch [1/10], Step [200/938], Loss: 2.2998\n",
            "Epoch [1/10], Step [300/938], Loss: 2.3068\n",
            "Epoch [1/10], Step [400/938], Loss: 2.2989\n",
            "Epoch [1/10], Step [500/938], Loss: 2.2913\n",
            "Epoch [1/10], Step [600/938], Loss: 2.2956\n",
            "Epoch [1/10], Step [700/938], Loss: 2.2873\n",
            "Epoch [1/10], Step [800/938], Loss: 2.2946\n",
            "Epoch [1/10], Step [900/938], Loss: 2.2915\n",
            "Epoch [2/10], Step [100/938], Loss: 2.2900\n",
            "Epoch [2/10], Step [200/938], Loss: 2.2698\n",
            "Epoch [2/10], Step [300/938], Loss: 2.2881\n",
            "Epoch [2/10], Step [400/938], Loss: 2.2969\n",
            "Epoch [2/10], Step [500/938], Loss: 2.2886\n",
            "Epoch [2/10], Step [600/938], Loss: 2.2871\n",
            "Epoch [2/10], Step [700/938], Loss: 2.2672\n",
            "Epoch [2/10], Step [800/938], Loss: 2.2881\n",
            "Epoch [2/10], Step [900/938], Loss: 2.2659\n",
            "Epoch [3/10], Step [100/938], Loss: 2.2770\n",
            "Epoch [3/10], Step [200/938], Loss: 2.2724\n",
            "Epoch [3/10], Step [300/938], Loss: 2.2742\n",
            "Epoch [3/10], Step [400/938], Loss: 2.2711\n",
            "Epoch [3/10], Step [500/938], Loss: 2.2624\n",
            "Epoch [3/10], Step [600/938], Loss: 2.2682\n",
            "Epoch [3/10], Step [700/938], Loss: 2.2706\n",
            "Epoch [3/10], Step [800/938], Loss: 2.2550\n",
            "Epoch [3/10], Step [900/938], Loss: 2.2422\n",
            "Epoch [4/10], Step [100/938], Loss: 2.2476\n",
            "Epoch [4/10], Step [200/938], Loss: 2.2381\n",
            "Epoch [4/10], Step [300/938], Loss: 2.2708\n",
            "Epoch [4/10], Step [400/938], Loss: 2.2504\n",
            "Epoch [4/10], Step [500/938], Loss: 2.2443\n",
            "Epoch [4/10], Step [600/938], Loss: 2.2452\n",
            "Epoch [4/10], Step [700/938], Loss: 2.2374\n",
            "Epoch [4/10], Step [800/938], Loss: 2.2105\n",
            "Epoch [4/10], Step [900/938], Loss: 2.2078\n",
            "Epoch [5/10], Step [100/938], Loss: 2.1711\n",
            "Epoch [5/10], Step [200/938], Loss: 2.1614\n",
            "Epoch [5/10], Step [300/938], Loss: 2.1778\n",
            "Epoch [5/10], Step [400/938], Loss: 2.2029\n",
            "Epoch [5/10], Step [500/938], Loss: 2.1878\n",
            "Epoch [5/10], Step [600/938], Loss: 2.1448\n",
            "Epoch [5/10], Step [700/938], Loss: 2.1487\n",
            "Epoch [5/10], Step [800/938], Loss: 2.1817\n",
            "Epoch [5/10], Step [900/938], Loss: 2.1525\n",
            "Epoch [6/10], Step [100/938], Loss: 2.1098\n",
            "Epoch [6/10], Step [200/938], Loss: 2.1622\n",
            "Epoch [6/10], Step [300/938], Loss: 2.1547\n",
            "Epoch [6/10], Step [400/938], Loss: 2.1309\n",
            "Epoch [6/10], Step [500/938], Loss: 2.1217\n",
            "Epoch [6/10], Step [600/938], Loss: 2.1087\n",
            "Epoch [6/10], Step [700/938], Loss: 2.0226\n",
            "Epoch [6/10], Step [800/938], Loss: 2.0226\n",
            "Epoch [6/10], Step [900/938], Loss: 2.0075\n",
            "Epoch [7/10], Step [100/938], Loss: 1.9230\n",
            "Epoch [7/10], Step [200/938], Loss: 2.0117\n",
            "Epoch [7/10], Step [300/938], Loss: 1.9936\n",
            "Epoch [7/10], Step [400/938], Loss: 1.9836\n",
            "Epoch [7/10], Step [500/938], Loss: 1.9306\n",
            "Epoch [7/10], Step [600/938], Loss: 1.9769\n",
            "Epoch [7/10], Step [700/938], Loss: 1.8790\n",
            "Epoch [7/10], Step [800/938], Loss: 1.9600\n",
            "Epoch [7/10], Step [900/938], Loss: 1.8685\n",
            "Epoch [8/10], Step [100/938], Loss: 1.8825\n",
            "Epoch [8/10], Step [200/938], Loss: 1.7950\n",
            "Epoch [8/10], Step [300/938], Loss: 1.9038\n",
            "Epoch [8/10], Step [400/938], Loss: 1.8569\n",
            "Epoch [8/10], Step [500/938], Loss: 1.8093\n",
            "Epoch [8/10], Step [600/938], Loss: 1.7975\n",
            "Epoch [8/10], Step [700/938], Loss: 1.7085\n",
            "Epoch [8/10], Step [800/938], Loss: 1.7912\n",
            "Epoch [8/10], Step [900/938], Loss: 1.6657\n",
            "Epoch [9/10], Step [100/938], Loss: 1.8584\n",
            "Epoch [9/10], Step [200/938], Loss: 1.6741\n",
            "Epoch [9/10], Step [300/938], Loss: 1.5710\n",
            "Epoch [9/10], Step [400/938], Loss: 1.5970\n",
            "Epoch [9/10], Step [500/938], Loss: 1.5273\n",
            "Epoch [9/10], Step [600/938], Loss: 1.6307\n",
            "Epoch [9/10], Step [700/938], Loss: 1.5638\n",
            "Epoch [9/10], Step [800/938], Loss: 1.5168\n",
            "Epoch [9/10], Step [900/938], Loss: 1.4641\n",
            "Epoch [10/10], Step [100/938], Loss: 1.4050\n",
            "Epoch [10/10], Step [200/938], Loss: 1.3464\n",
            "Epoch [10/10], Step [300/938], Loss: 1.4332\n",
            "Epoch [10/10], Step [400/938], Loss: 1.3437\n",
            "Epoch [10/10], Step [500/938], Loss: 1.3279\n",
            "Epoch [10/10], Step [600/938], Loss: 1.3757\n",
            "Epoch [10/10], Step [700/938], Loss: 1.3028\n",
            "Epoch [10/10], Step [800/938], Loss: 1.2100\n",
            "Epoch [10/10], Step [900/938], Loss: 1.3329\n",
            "Accuracy of the network on the 10000 test images: 61.29 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the network and define the loss function and optimizer\n",
        "net = model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader_train):\n",
        "        images, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(dataloader_train), loss.item()))\n",
        "\n",
        "# Test the network\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in dataloader_test:\n",
        "        images, labels = data\n",
        "        outputs = net(images.float())\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWfN9bbHivBM",
        "outputId": "5bb9f2c9-ae2f-440b-802d-b6e0e9e6330b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [100/938], Loss: 2.3136\n",
            "Epoch [1/100], Step [200/938], Loss: 2.3066\n",
            "Epoch [1/100], Step [300/938], Loss: 2.2929\n",
            "Epoch [1/100], Step [400/938], Loss: 2.2980\n",
            "Epoch [1/100], Step [500/938], Loss: 2.3019\n",
            "Epoch [1/100], Step [600/938], Loss: 2.2887\n",
            "Epoch [1/100], Step [700/938], Loss: 2.2921\n",
            "Epoch [1/100], Step [800/938], Loss: 2.2917\n",
            "Epoch [1/100], Step [900/938], Loss: 2.2995\n",
            "Epoch [2/100], Step [100/938], Loss: 2.2841\n",
            "Epoch [2/100], Step [200/938], Loss: 2.2905\n",
            "Epoch [2/100], Step [300/938], Loss: 2.2837\n",
            "Epoch [2/100], Step [400/938], Loss: 2.2852\n",
            "Epoch [2/100], Step [500/938], Loss: 2.2886\n",
            "Epoch [2/100], Step [600/938], Loss: 2.2839\n",
            "Epoch [2/100], Step [700/938], Loss: 2.2807\n",
            "Epoch [2/100], Step [800/938], Loss: 2.2691\n",
            "Epoch [2/100], Step [900/938], Loss: 2.2857\n",
            "Epoch [3/100], Step [100/938], Loss: 2.2703\n",
            "Epoch [3/100], Step [200/938], Loss: 2.2776\n",
            "Epoch [3/100], Step [300/938], Loss: 2.2673\n",
            "Epoch [3/100], Step [400/938], Loss: 2.2757\n",
            "Epoch [3/100], Step [500/938], Loss: 2.2601\n",
            "Epoch [3/100], Step [600/938], Loss: 2.2780\n",
            "Epoch [3/100], Step [700/938], Loss: 2.2797\n",
            "Epoch [3/100], Step [800/938], Loss: 2.2527\n",
            "Epoch [3/100], Step [900/938], Loss: 2.2654\n",
            "Epoch [4/100], Step [100/938], Loss: 2.2544\n",
            "Epoch [4/100], Step [200/938], Loss: 2.2617\n",
            "Epoch [4/100], Step [300/938], Loss: 2.2345\n",
            "Epoch [4/100], Step [400/938], Loss: 2.2626\n",
            "Epoch [4/100], Step [500/938], Loss: 2.2347\n",
            "Epoch [4/100], Step [600/938], Loss: 2.2410\n",
            "Epoch [4/100], Step [700/938], Loss: 2.2437\n",
            "Epoch [4/100], Step [800/938], Loss: 2.2276\n",
            "Epoch [4/100], Step [900/938], Loss: 2.2161\n",
            "Epoch [5/100], Step [100/938], Loss: 2.2219\n",
            "Epoch [5/100], Step [200/938], Loss: 2.2176\n",
            "Epoch [5/100], Step [300/938], Loss: 2.2011\n",
            "Epoch [5/100], Step [400/938], Loss: 2.2049\n",
            "Epoch [5/100], Step [500/938], Loss: 2.1809\n",
            "Epoch [5/100], Step [600/938], Loss: 2.1644\n",
            "Epoch [5/100], Step [700/938], Loss: 2.1479\n",
            "Epoch [5/100], Step [800/938], Loss: 2.1755\n",
            "Epoch [5/100], Step [900/938], Loss: 2.1375\n",
            "Epoch [6/100], Step [100/938], Loss: 2.1630\n",
            "Epoch [6/100], Step [200/938], Loss: 2.1264\n",
            "Epoch [6/100], Step [300/938], Loss: 2.0890\n",
            "Epoch [6/100], Step [400/938], Loss: 2.1136\n",
            "Epoch [6/100], Step [500/938], Loss: 2.1120\n",
            "Epoch [6/100], Step [600/938], Loss: 2.0898\n",
            "Epoch [6/100], Step [700/938], Loss: 2.0823\n",
            "Epoch [6/100], Step [800/938], Loss: 2.0930\n",
            "Epoch [6/100], Step [900/938], Loss: 2.1141\n",
            "Epoch [7/100], Step [100/938], Loss: 2.0277\n",
            "Epoch [7/100], Step [200/938], Loss: 1.9472\n",
            "Epoch [7/100], Step [300/938], Loss: 2.0025\n",
            "Epoch [7/100], Step [400/938], Loss: 1.9047\n",
            "Epoch [7/100], Step [500/938], Loss: 2.0188\n",
            "Epoch [7/100], Step [600/938], Loss: 1.9092\n",
            "Epoch [7/100], Step [700/938], Loss: 1.9518\n",
            "Epoch [7/100], Step [800/938], Loss: 1.9432\n",
            "Epoch [7/100], Step [900/938], Loss: 1.8043\n",
            "Epoch [8/100], Step [100/938], Loss: 1.8744\n",
            "Epoch [8/100], Step [200/938], Loss: 1.8920\n",
            "Epoch [8/100], Step [300/938], Loss: 1.8067\n",
            "Epoch [8/100], Step [400/938], Loss: 1.7525\n",
            "Epoch [8/100], Step [500/938], Loss: 1.8403\n",
            "Epoch [8/100], Step [600/938], Loss: 1.7490\n",
            "Epoch [8/100], Step [700/938], Loss: 1.8256\n",
            "Epoch [8/100], Step [800/938], Loss: 1.7341\n",
            "Epoch [8/100], Step [900/938], Loss: 1.6568\n",
            "Epoch [9/100], Step [100/938], Loss: 1.6035\n",
            "Epoch [9/100], Step [200/938], Loss: 1.6481\n",
            "Epoch [9/100], Step [300/938], Loss: 1.6625\n",
            "Epoch [9/100], Step [400/938], Loss: 1.5800\n",
            "Epoch [9/100], Step [500/938], Loss: 1.7378\n",
            "Epoch [9/100], Step [600/938], Loss: 1.5717\n",
            "Epoch [9/100], Step [700/938], Loss: 1.4823\n",
            "Epoch [9/100], Step [800/938], Loss: 1.4674\n",
            "Epoch [9/100], Step [900/938], Loss: 1.4523\n",
            "Epoch [10/100], Step [100/938], Loss: 1.3859\n",
            "Epoch [10/100], Step [200/938], Loss: 1.4039\n",
            "Epoch [10/100], Step [300/938], Loss: 1.3602\n",
            "Epoch [10/100], Step [400/938], Loss: 1.4278\n",
            "Epoch [10/100], Step [500/938], Loss: 1.3934\n",
            "Epoch [10/100], Step [600/938], Loss: 1.3104\n",
            "Epoch [10/100], Step [700/938], Loss: 1.3300\n",
            "Epoch [10/100], Step [800/938], Loss: 1.2680\n",
            "Epoch [10/100], Step [900/938], Loss: 1.1059\n",
            "Epoch [11/100], Step [100/938], Loss: 1.2259\n",
            "Epoch [11/100], Step [200/938], Loss: 1.2118\n",
            "Epoch [11/100], Step [300/938], Loss: 1.0934\n",
            "Epoch [11/100], Step [400/938], Loss: 1.1103\n",
            "Epoch [11/100], Step [500/938], Loss: 1.1442\n",
            "Epoch [11/100], Step [600/938], Loss: 1.0641\n",
            "Epoch [11/100], Step [700/938], Loss: 1.0165\n",
            "Epoch [11/100], Step [800/938], Loss: 1.1432\n",
            "Epoch [11/100], Step [900/938], Loss: 0.9959\n",
            "Epoch [12/100], Step [100/938], Loss: 0.9559\n",
            "Epoch [12/100], Step [200/938], Loss: 1.0159\n",
            "Epoch [12/100], Step [300/938], Loss: 0.9475\n",
            "Epoch [12/100], Step [400/938], Loss: 0.9111\n",
            "Epoch [12/100], Step [500/938], Loss: 0.7689\n",
            "Epoch [12/100], Step [600/938], Loss: 0.8121\n",
            "Epoch [12/100], Step [700/938], Loss: 0.9261\n",
            "Epoch [12/100], Step [800/938], Loss: 0.7511\n",
            "Epoch [12/100], Step [900/938], Loss: 0.7996\n",
            "Epoch [13/100], Step [100/938], Loss: 0.7591\n",
            "Epoch [13/100], Step [200/938], Loss: 0.8539\n",
            "Epoch [13/100], Step [300/938], Loss: 0.7805\n",
            "Epoch [13/100], Step [400/938], Loss: 0.7730\n",
            "Epoch [13/100], Step [500/938], Loss: 0.8956\n",
            "Epoch [13/100], Step [600/938], Loss: 0.5277\n",
            "Epoch [13/100], Step [700/938], Loss: 0.8320\n",
            "Epoch [13/100], Step [800/938], Loss: 0.7419\n",
            "Epoch [13/100], Step [900/938], Loss: 0.7239\n",
            "Epoch [14/100], Step [100/938], Loss: 0.4920\n",
            "Epoch [14/100], Step [200/938], Loss: 0.6403\n",
            "Epoch [14/100], Step [300/938], Loss: 0.5609\n",
            "Epoch [14/100], Step [400/938], Loss: 0.6858\n",
            "Epoch [14/100], Step [500/938], Loss: 0.7729\n",
            "Epoch [14/100], Step [600/938], Loss: 0.6268\n",
            "Epoch [14/100], Step [700/938], Loss: 0.6036\n",
            "Epoch [14/100], Step [800/938], Loss: 0.5116\n",
            "Epoch [14/100], Step [900/938], Loss: 0.5018\n",
            "Epoch [15/100], Step [100/938], Loss: 0.4460\n",
            "Epoch [15/100], Step [200/938], Loss: 0.5072\n",
            "Epoch [15/100], Step [300/938], Loss: 0.5441\n",
            "Epoch [15/100], Step [400/938], Loss: 0.5772\n",
            "Epoch [15/100], Step [500/938], Loss: 0.6841\n",
            "Epoch [15/100], Step [600/938], Loss: 0.4498\n",
            "Epoch [15/100], Step [700/938], Loss: 0.5074\n",
            "Epoch [15/100], Step [800/938], Loss: 0.4466\n",
            "Epoch [15/100], Step [900/938], Loss: 0.5836\n",
            "Epoch [16/100], Step [100/938], Loss: 0.5691\n",
            "Epoch [16/100], Step [200/938], Loss: 0.3943\n",
            "Epoch [16/100], Step [300/938], Loss: 0.5007\n",
            "Epoch [16/100], Step [400/938], Loss: 0.4677\n",
            "Epoch [16/100], Step [500/938], Loss: 0.4841\n",
            "Epoch [16/100], Step [600/938], Loss: 0.5330\n",
            "Epoch [16/100], Step [700/938], Loss: 0.5155\n",
            "Epoch [16/100], Step [800/938], Loss: 0.6425\n",
            "Epoch [16/100], Step [900/938], Loss: 0.4782\n",
            "Epoch [17/100], Step [100/938], Loss: 0.3543\n",
            "Epoch [17/100], Step [200/938], Loss: 0.6908\n",
            "Epoch [17/100], Step [300/938], Loss: 0.4071\n",
            "Epoch [17/100], Step [400/938], Loss: 0.4742\n",
            "Epoch [17/100], Step [500/938], Loss: 0.3625\n",
            "Epoch [17/100], Step [600/938], Loss: 0.5342\n",
            "Epoch [17/100], Step [700/938], Loss: 0.6932\n",
            "Epoch [17/100], Step [800/938], Loss: 0.3366\n",
            "Epoch [17/100], Step [900/938], Loss: 0.3526\n",
            "Epoch [18/100], Step [100/938], Loss: 0.2983\n",
            "Epoch [18/100], Step [200/938], Loss: 0.4127\n",
            "Epoch [18/100], Step [300/938], Loss: 0.4763\n",
            "Epoch [18/100], Step [400/938], Loss: 0.4179\n",
            "Epoch [18/100], Step [500/938], Loss: 0.2128\n",
            "Epoch [18/100], Step [600/938], Loss: 0.3700\n",
            "Epoch [18/100], Step [700/938], Loss: 0.2962\n",
            "Epoch [18/100], Step [800/938], Loss: 0.4109\n",
            "Epoch [18/100], Step [900/938], Loss: 0.3078\n",
            "Epoch [19/100], Step [100/938], Loss: 0.4561\n",
            "Epoch [19/100], Step [200/938], Loss: 0.4994\n",
            "Epoch [19/100], Step [300/938], Loss: 0.3317\n",
            "Epoch [19/100], Step [400/938], Loss: 0.3160\n",
            "Epoch [19/100], Step [500/938], Loss: 0.2588\n",
            "Epoch [19/100], Step [600/938], Loss: 0.3591\n",
            "Epoch [19/100], Step [700/938], Loss: 0.2851\n",
            "Epoch [19/100], Step [800/938], Loss: 0.3166\n",
            "Epoch [19/100], Step [900/938], Loss: 0.5050\n",
            "Epoch [20/100], Step [100/938], Loss: 0.3584\n",
            "Epoch [20/100], Step [200/938], Loss: 0.7125\n",
            "Epoch [20/100], Step [300/938], Loss: 0.5810\n",
            "Epoch [20/100], Step [400/938], Loss: 0.4709\n",
            "Epoch [20/100], Step [500/938], Loss: 0.3892\n",
            "Epoch [20/100], Step [600/938], Loss: 0.5244\n",
            "Epoch [20/100], Step [700/938], Loss: 0.4011\n",
            "Epoch [20/100], Step [800/938], Loss: 0.3956\n",
            "Epoch [20/100], Step [900/938], Loss: 0.3652\n",
            "Epoch [21/100], Step [100/938], Loss: 0.3772\n",
            "Epoch [21/100], Step [200/938], Loss: 0.2565\n",
            "Epoch [21/100], Step [300/938], Loss: 0.4079\n",
            "Epoch [21/100], Step [400/938], Loss: 0.4077\n",
            "Epoch [21/100], Step [500/938], Loss: 0.2009\n",
            "Epoch [21/100], Step [600/938], Loss: 0.2959\n",
            "Epoch [21/100], Step [700/938], Loss: 0.3826\n",
            "Epoch [21/100], Step [800/938], Loss: 0.2436\n",
            "Epoch [21/100], Step [900/938], Loss: 0.4469\n",
            "Epoch [22/100], Step [100/938], Loss: 0.3409\n",
            "Epoch [22/100], Step [200/938], Loss: 0.3724\n",
            "Epoch [22/100], Step [300/938], Loss: 0.3090\n",
            "Epoch [22/100], Step [400/938], Loss: 0.2606\n",
            "Epoch [22/100], Step [500/938], Loss: 0.4111\n",
            "Epoch [22/100], Step [600/938], Loss: 0.4074\n",
            "Epoch [22/100], Step [700/938], Loss: 0.3365\n",
            "Epoch [22/100], Step [800/938], Loss: 0.4501\n",
            "Epoch [22/100], Step [900/938], Loss: 0.2505\n",
            "Epoch [23/100], Step [100/938], Loss: 0.2177\n",
            "Epoch [23/100], Step [200/938], Loss: 0.3932\n",
            "Epoch [23/100], Step [300/938], Loss: 0.3377\n",
            "Epoch [23/100], Step [400/938], Loss: 0.3178\n",
            "Epoch [23/100], Step [500/938], Loss: 0.3743\n",
            "Epoch [23/100], Step [600/938], Loss: 0.4752\n",
            "Epoch [23/100], Step [700/938], Loss: 0.3764\n",
            "Epoch [23/100], Step [800/938], Loss: 0.1515\n",
            "Epoch [23/100], Step [900/938], Loss: 0.4570\n",
            "Epoch [24/100], Step [100/938], Loss: 0.2958\n",
            "Epoch [24/100], Step [200/938], Loss: 0.2377\n",
            "Epoch [24/100], Step [300/938], Loss: 0.3528\n",
            "Epoch [24/100], Step [400/938], Loss: 0.3527\n",
            "Epoch [24/100], Step [500/938], Loss: 0.2522\n",
            "Epoch [24/100], Step [600/938], Loss: 0.2889\n",
            "Epoch [24/100], Step [700/938], Loss: 0.3957\n",
            "Epoch [24/100], Step [800/938], Loss: 0.1482\n",
            "Epoch [24/100], Step [900/938], Loss: 0.4018\n",
            "Epoch [25/100], Step [100/938], Loss: 0.2242\n",
            "Epoch [25/100], Step [200/938], Loss: 0.4474\n",
            "Epoch [25/100], Step [300/938], Loss: 0.2526\n",
            "Epoch [25/100], Step [400/938], Loss: 0.2441\n",
            "Epoch [25/100], Step [500/938], Loss: 0.4474\n",
            "Epoch [25/100], Step [600/938], Loss: 0.2504\n",
            "Epoch [25/100], Step [700/938], Loss: 0.2186\n",
            "Epoch [25/100], Step [800/938], Loss: 0.2645\n",
            "Epoch [25/100], Step [900/938], Loss: 0.3879\n",
            "Epoch [26/100], Step [100/938], Loss: 0.2624\n",
            "Epoch [26/100], Step [200/938], Loss: 0.1993\n",
            "Epoch [26/100], Step [300/938], Loss: 0.3629\n",
            "Epoch [26/100], Step [400/938], Loss: 0.1641\n",
            "Epoch [26/100], Step [500/938], Loss: 0.1330\n",
            "Epoch [26/100], Step [600/938], Loss: 0.3148\n",
            "Epoch [26/100], Step [700/938], Loss: 0.2696\n",
            "Epoch [26/100], Step [800/938], Loss: 0.0865\n",
            "Epoch [26/100], Step [900/938], Loss: 0.2149\n",
            "Epoch [27/100], Step [100/938], Loss: 0.4120\n",
            "Epoch [27/100], Step [200/938], Loss: 0.4804\n",
            "Epoch [27/100], Step [300/938], Loss: 0.2639\n",
            "Epoch [27/100], Step [400/938], Loss: 0.2144\n",
            "Epoch [27/100], Step [500/938], Loss: 0.3141\n",
            "Epoch [27/100], Step [600/938], Loss: 0.3114\n",
            "Epoch [27/100], Step [700/938], Loss: 0.3356\n",
            "Epoch [27/100], Step [800/938], Loss: 0.2600\n",
            "Epoch [27/100], Step [900/938], Loss: 0.3629\n",
            "Epoch [28/100], Step [100/938], Loss: 0.2882\n",
            "Epoch [28/100], Step [200/938], Loss: 0.1969\n",
            "Epoch [28/100], Step [300/938], Loss: 0.2514\n",
            "Epoch [28/100], Step [400/938], Loss: 0.2640\n",
            "Epoch [28/100], Step [500/938], Loss: 0.3260\n",
            "Epoch [28/100], Step [600/938], Loss: 0.1356\n",
            "Epoch [28/100], Step [700/938], Loss: 0.2329\n",
            "Epoch [28/100], Step [800/938], Loss: 0.2324\n",
            "Epoch [28/100], Step [900/938], Loss: 0.2748\n",
            "Epoch [29/100], Step [100/938], Loss: 0.3223\n",
            "Epoch [29/100], Step [200/938], Loss: 0.2614\n",
            "Epoch [29/100], Step [300/938], Loss: 0.5085\n",
            "Epoch [29/100], Step [400/938], Loss: 0.3341\n",
            "Epoch [29/100], Step [500/938], Loss: 0.3597\n",
            "Epoch [29/100], Step [600/938], Loss: 0.2440\n",
            "Epoch [29/100], Step [700/938], Loss: 0.1497\n",
            "Epoch [29/100], Step [800/938], Loss: 0.2130\n",
            "Epoch [29/100], Step [900/938], Loss: 0.1863\n",
            "Epoch [30/100], Step [100/938], Loss: 0.4636\n",
            "Epoch [30/100], Step [200/938], Loss: 0.2918\n",
            "Epoch [30/100], Step [300/938], Loss: 0.2640\n",
            "Epoch [30/100], Step [400/938], Loss: 0.1509\n",
            "Epoch [30/100], Step [500/938], Loss: 0.4317\n",
            "Epoch [30/100], Step [600/938], Loss: 0.2000\n",
            "Epoch [30/100], Step [700/938], Loss: 0.2031\n",
            "Epoch [30/100], Step [800/938], Loss: 0.5315\n",
            "Epoch [30/100], Step [900/938], Loss: 0.1535\n",
            "Epoch [31/100], Step [100/938], Loss: 0.1880\n",
            "Epoch [31/100], Step [200/938], Loss: 0.3176\n",
            "Epoch [31/100], Step [300/938], Loss: 0.1385\n",
            "Epoch [31/100], Step [400/938], Loss: 0.1972\n",
            "Epoch [31/100], Step [500/938], Loss: 0.3495\n",
            "Epoch [31/100], Step [600/938], Loss: 0.2458\n",
            "Epoch [31/100], Step [700/938], Loss: 0.1671\n",
            "Epoch [31/100], Step [800/938], Loss: 0.2548\n",
            "Epoch [31/100], Step [900/938], Loss: 0.2147\n",
            "Epoch [32/100], Step [100/938], Loss: 0.3295\n",
            "Epoch [32/100], Step [200/938], Loss: 0.1944\n",
            "Epoch [32/100], Step [300/938], Loss: 0.2008\n",
            "Epoch [32/100], Step [400/938], Loss: 0.4314\n",
            "Epoch [32/100], Step [500/938], Loss: 0.3495\n",
            "Epoch [32/100], Step [600/938], Loss: 0.3958\n",
            "Epoch [32/100], Step [700/938], Loss: 0.1537\n",
            "Epoch [32/100], Step [800/938], Loss: 0.1097\n",
            "Epoch [32/100], Step [900/938], Loss: 0.2139\n",
            "Epoch [33/100], Step [100/938], Loss: 0.1999\n",
            "Epoch [33/100], Step [200/938], Loss: 0.1790\n",
            "Epoch [33/100], Step [300/938], Loss: 0.0904\n",
            "Epoch [33/100], Step [400/938], Loss: 0.1678\n",
            "Epoch [33/100], Step [500/938], Loss: 0.1842\n",
            "Epoch [33/100], Step [600/938], Loss: 0.2568\n",
            "Epoch [33/100], Step [700/938], Loss: 0.1739\n",
            "Epoch [33/100], Step [800/938], Loss: 0.2393\n",
            "Epoch [33/100], Step [900/938], Loss: 0.2157\n",
            "Epoch [34/100], Step [100/938], Loss: 0.0931\n",
            "Epoch [34/100], Step [200/938], Loss: 0.1892\n",
            "Epoch [34/100], Step [300/938], Loss: 0.2584\n",
            "Epoch [34/100], Step [400/938], Loss: 0.2234\n",
            "Epoch [34/100], Step [500/938], Loss: 0.2383\n",
            "Epoch [34/100], Step [600/938], Loss: 0.4009\n",
            "Epoch [34/100], Step [700/938], Loss: 0.1107\n",
            "Epoch [34/100], Step [800/938], Loss: 0.2671\n",
            "Epoch [34/100], Step [900/938], Loss: 0.1459\n",
            "Epoch [35/100], Step [100/938], Loss: 0.2360\n",
            "Epoch [35/100], Step [200/938], Loss: 0.2269\n",
            "Epoch [35/100], Step [300/938], Loss: 0.3113\n",
            "Epoch [35/100], Step [400/938], Loss: 0.1716\n",
            "Epoch [35/100], Step [500/938], Loss: 0.1682\n",
            "Epoch [35/100], Step [600/938], Loss: 0.1988\n",
            "Epoch [35/100], Step [700/938], Loss: 0.1935\n",
            "Epoch [35/100], Step [800/938], Loss: 0.2322\n",
            "Epoch [35/100], Step [900/938], Loss: 0.3800\n",
            "Epoch [36/100], Step [100/938], Loss: 0.2340\n",
            "Epoch [36/100], Step [200/938], Loss: 0.1564\n",
            "Epoch [36/100], Step [300/938], Loss: 0.1058\n",
            "Epoch [36/100], Step [400/938], Loss: 0.2447\n",
            "Epoch [36/100], Step [500/938], Loss: 0.1700\n",
            "Epoch [36/100], Step [600/938], Loss: 0.2098\n",
            "Epoch [36/100], Step [700/938], Loss: 0.3260\n",
            "Epoch [36/100], Step [800/938], Loss: 0.1989\n",
            "Epoch [36/100], Step [900/938], Loss: 0.2002\n",
            "Epoch [37/100], Step [100/938], Loss: 0.1287\n",
            "Epoch [37/100], Step [200/938], Loss: 0.1406\n",
            "Epoch [37/100], Step [300/938], Loss: 0.2603\n",
            "Epoch [37/100], Step [400/938], Loss: 0.0723\n",
            "Epoch [37/100], Step [500/938], Loss: 0.2150\n",
            "Epoch [37/100], Step [600/938], Loss: 0.3267\n",
            "Epoch [37/100], Step [700/938], Loss: 0.1735\n",
            "Epoch [37/100], Step [800/938], Loss: 0.2382\n",
            "Epoch [37/100], Step [900/938], Loss: 0.2991\n",
            "Epoch [38/100], Step [100/938], Loss: 0.0923\n",
            "Epoch [38/100], Step [200/938], Loss: 0.1187\n",
            "Epoch [38/100], Step [300/938], Loss: 0.1301\n",
            "Epoch [38/100], Step [400/938], Loss: 0.1910\n",
            "Epoch [38/100], Step [500/938], Loss: 0.2491\n",
            "Epoch [38/100], Step [600/938], Loss: 0.1738\n",
            "Epoch [38/100], Step [700/938], Loss: 0.1104\n",
            "Epoch [38/100], Step [800/938], Loss: 0.2044\n",
            "Epoch [38/100], Step [900/938], Loss: 0.1412\n",
            "Epoch [39/100], Step [100/938], Loss: 0.2462\n",
            "Epoch [39/100], Step [200/938], Loss: 0.2823\n",
            "Epoch [39/100], Step [300/938], Loss: 0.1121\n",
            "Epoch [39/100], Step [400/938], Loss: 0.1477\n",
            "Epoch [39/100], Step [500/938], Loss: 0.1933\n",
            "Epoch [39/100], Step [600/938], Loss: 0.2227\n",
            "Epoch [39/100], Step [700/938], Loss: 0.2145\n",
            "Epoch [39/100], Step [800/938], Loss: 0.1035\n",
            "Epoch [39/100], Step [900/938], Loss: 0.1814\n",
            "Epoch [40/100], Step [100/938], Loss: 0.1960\n",
            "Epoch [40/100], Step [200/938], Loss: 0.1020\n",
            "Epoch [40/100], Step [300/938], Loss: 0.1372\n",
            "Epoch [40/100], Step [400/938], Loss: 0.0877\n",
            "Epoch [40/100], Step [500/938], Loss: 0.2017\n",
            "Epoch [40/100], Step [600/938], Loss: 0.2062\n",
            "Epoch [40/100], Step [700/938], Loss: 0.3964\n",
            "Epoch [40/100], Step [800/938], Loss: 0.1849\n",
            "Epoch [40/100], Step [900/938], Loss: 0.1238\n",
            "Epoch [41/100], Step [100/938], Loss: 0.2498\n",
            "Epoch [41/100], Step [200/938], Loss: 0.1955\n",
            "Epoch [41/100], Step [300/938], Loss: 0.2188\n",
            "Epoch [41/100], Step [400/938], Loss: 0.1509\n",
            "Epoch [41/100], Step [500/938], Loss: 0.0816\n",
            "Epoch [41/100], Step [600/938], Loss: 0.2441\n",
            "Epoch [41/100], Step [700/938], Loss: 0.2105\n",
            "Epoch [41/100], Step [800/938], Loss: 0.5305\n",
            "Epoch [41/100], Step [900/938], Loss: 0.1853\n",
            "Epoch [42/100], Step [100/938], Loss: 0.2655\n",
            "Epoch [42/100], Step [200/938], Loss: 0.2961\n",
            "Epoch [42/100], Step [300/938], Loss: 0.1601\n",
            "Epoch [42/100], Step [400/938], Loss: 0.1117\n",
            "Epoch [42/100], Step [500/938], Loss: 0.1241\n",
            "Epoch [42/100], Step [600/938], Loss: 0.1274\n",
            "Epoch [42/100], Step [700/938], Loss: 0.3136\n",
            "Epoch [42/100], Step [800/938], Loss: 0.1716\n",
            "Epoch [42/100], Step [900/938], Loss: 0.1340\n",
            "Epoch [43/100], Step [100/938], Loss: 0.2218\n",
            "Epoch [43/100], Step [200/938], Loss: 0.1630\n",
            "Epoch [43/100], Step [300/938], Loss: 0.1766\n",
            "Epoch [43/100], Step [400/938], Loss: 0.1610\n",
            "Epoch [43/100], Step [500/938], Loss: 0.2114\n",
            "Epoch [43/100], Step [600/938], Loss: 0.1750\n",
            "Epoch [43/100], Step [700/938], Loss: 0.2570\n",
            "Epoch [43/100], Step [800/938], Loss: 0.0837\n",
            "Epoch [43/100], Step [900/938], Loss: 0.1757\n",
            "Epoch [44/100], Step [100/938], Loss: 0.1013\n",
            "Epoch [44/100], Step [200/938], Loss: 0.2521\n",
            "Epoch [44/100], Step [300/938], Loss: 0.0619\n",
            "Epoch [44/100], Step [400/938], Loss: 0.1360\n",
            "Epoch [44/100], Step [500/938], Loss: 0.1571\n",
            "Epoch [44/100], Step [600/938], Loss: 0.3793\n",
            "Epoch [44/100], Step [700/938], Loss: 0.1434\n",
            "Epoch [44/100], Step [800/938], Loss: 0.2265\n",
            "Epoch [44/100], Step [900/938], Loss: 0.1395\n",
            "Epoch [45/100], Step [100/938], Loss: 0.2227\n",
            "Epoch [45/100], Step [200/938], Loss: 0.0753\n",
            "Epoch [45/100], Step [300/938], Loss: 0.1511\n",
            "Epoch [45/100], Step [400/938], Loss: 0.2792\n",
            "Epoch [45/100], Step [500/938], Loss: 0.1761\n",
            "Epoch [45/100], Step [600/938], Loss: 0.2820\n",
            "Epoch [45/100], Step [700/938], Loss: 0.1025\n",
            "Epoch [45/100], Step [800/938], Loss: 0.1658\n",
            "Epoch [45/100], Step [900/938], Loss: 0.0953\n",
            "Epoch [46/100], Step [100/938], Loss: 0.2309\n",
            "Epoch [46/100], Step [200/938], Loss: 0.1264\n",
            "Epoch [46/100], Step [300/938], Loss: 0.1585\n",
            "Epoch [46/100], Step [400/938], Loss: 0.1397\n",
            "Epoch [46/100], Step [500/938], Loss: 0.1081\n",
            "Epoch [46/100], Step [600/938], Loss: 0.1084\n",
            "Epoch [46/100], Step [700/938], Loss: 0.1737\n",
            "Epoch [46/100], Step [800/938], Loss: 0.1566\n",
            "Epoch [46/100], Step [900/938], Loss: 0.0912\n",
            "Epoch [47/100], Step [100/938], Loss: 0.0641\n",
            "Epoch [47/100], Step [200/938], Loss: 0.2316\n",
            "Epoch [47/100], Step [300/938], Loss: 0.1790\n",
            "Epoch [47/100], Step [400/938], Loss: 0.0940\n",
            "Epoch [47/100], Step [500/938], Loss: 0.1624\n",
            "Epoch [47/100], Step [600/938], Loss: 0.2019\n",
            "Epoch [47/100], Step [700/938], Loss: 0.1097\n",
            "Epoch [47/100], Step [800/938], Loss: 0.1718\n",
            "Epoch [47/100], Step [900/938], Loss: 0.0505\n",
            "Epoch [48/100], Step [100/938], Loss: 0.2438\n",
            "Epoch [48/100], Step [200/938], Loss: 0.1399\n",
            "Epoch [48/100], Step [300/938], Loss: 0.1102\n",
            "Epoch [48/100], Step [400/938], Loss: 0.1230\n",
            "Epoch [48/100], Step [500/938], Loss: 0.2155\n",
            "Epoch [48/100], Step [600/938], Loss: 0.0572\n",
            "Epoch [48/100], Step [700/938], Loss: 0.3153\n",
            "Epoch [48/100], Step [800/938], Loss: 0.2396\n",
            "Epoch [48/100], Step [900/938], Loss: 0.0667\n",
            "Epoch [49/100], Step [100/938], Loss: 0.1252\n",
            "Epoch [49/100], Step [200/938], Loss: 0.1257\n",
            "Epoch [49/100], Step [300/938], Loss: 0.0752\n",
            "Epoch [49/100], Step [400/938], Loss: 0.0912\n",
            "Epoch [49/100], Step [500/938], Loss: 0.0981\n",
            "Epoch [49/100], Step [600/938], Loss: 0.2062\n",
            "Epoch [49/100], Step [700/938], Loss: 0.1799\n",
            "Epoch [49/100], Step [800/938], Loss: 0.3135\n",
            "Epoch [49/100], Step [900/938], Loss: 0.0739\n",
            "Epoch [50/100], Step [100/938], Loss: 0.2270\n",
            "Epoch [50/100], Step [200/938], Loss: 0.1297\n",
            "Epoch [50/100], Step [300/938], Loss: 0.0731\n",
            "Epoch [50/100], Step [400/938], Loss: 0.1240\n",
            "Epoch [50/100], Step [500/938], Loss: 0.1972\n",
            "Epoch [50/100], Step [600/938], Loss: 0.1682\n",
            "Epoch [50/100], Step [700/938], Loss: 0.2151\n",
            "Epoch [50/100], Step [800/938], Loss: 0.1295\n",
            "Epoch [50/100], Step [900/938], Loss: 0.3033\n",
            "Epoch [51/100], Step [100/938], Loss: 0.2006\n",
            "Epoch [51/100], Step [200/938], Loss: 0.1003\n",
            "Epoch [51/100], Step [300/938], Loss: 0.1705\n",
            "Epoch [51/100], Step [400/938], Loss: 0.2014\n",
            "Epoch [51/100], Step [500/938], Loss: 0.1066\n",
            "Epoch [51/100], Step [600/938], Loss: 0.0846\n",
            "Epoch [51/100], Step [700/938], Loss: 0.1171\n",
            "Epoch [51/100], Step [800/938], Loss: 0.1035\n",
            "Epoch [51/100], Step [900/938], Loss: 0.1594\n",
            "Epoch [52/100], Step [100/938], Loss: 0.0959\n",
            "Epoch [52/100], Step [200/938], Loss: 0.0650\n",
            "Epoch [52/100], Step [300/938], Loss: 0.1452\n",
            "Epoch [52/100], Step [400/938], Loss: 0.1321\n",
            "Epoch [52/100], Step [500/938], Loss: 0.1305\n",
            "Epoch [52/100], Step [600/938], Loss: 0.0925\n",
            "Epoch [52/100], Step [700/938], Loss: 0.2454\n",
            "Epoch [52/100], Step [800/938], Loss: 0.0535\n",
            "Epoch [52/100], Step [900/938], Loss: 0.0893\n",
            "Epoch [53/100], Step [100/938], Loss: 0.1116\n",
            "Epoch [53/100], Step [200/938], Loss: 0.2046\n",
            "Epoch [53/100], Step [300/938], Loss: 0.1552\n",
            "Epoch [53/100], Step [400/938], Loss: 0.1643\n",
            "Epoch [53/100], Step [500/938], Loss: 0.2884\n",
            "Epoch [53/100], Step [600/938], Loss: 0.1797\n",
            "Epoch [53/100], Step [700/938], Loss: 0.1120\n",
            "Epoch [53/100], Step [800/938], Loss: 0.1112\n",
            "Epoch [53/100], Step [900/938], Loss: 0.1078\n",
            "Epoch [54/100], Step [100/938], Loss: 0.0569\n",
            "Epoch [54/100], Step [200/938], Loss: 0.2335\n",
            "Epoch [54/100], Step [300/938], Loss: 0.1359\n",
            "Epoch [54/100], Step [400/938], Loss: 0.0967\n",
            "Epoch [54/100], Step [500/938], Loss: 0.0756\n",
            "Epoch [54/100], Step [600/938], Loss: 0.1046\n",
            "Epoch [54/100], Step [700/938], Loss: 0.0782\n",
            "Epoch [54/100], Step [800/938], Loss: 0.1261\n",
            "Epoch [54/100], Step [900/938], Loss: 0.0774\n",
            "Epoch [55/100], Step [100/938], Loss: 0.0507\n",
            "Epoch [55/100], Step [200/938], Loss: 0.1296\n",
            "Epoch [55/100], Step [300/938], Loss: 0.1535\n",
            "Epoch [55/100], Step [400/938], Loss: 0.0783\n",
            "Epoch [55/100], Step [500/938], Loss: 0.0781\n",
            "Epoch [55/100], Step [600/938], Loss: 0.0754\n",
            "Epoch [55/100], Step [700/938], Loss: 0.1492\n",
            "Epoch [55/100], Step [800/938], Loss: 0.1460\n",
            "Epoch [55/100], Step [900/938], Loss: 0.1547\n",
            "Epoch [56/100], Step [100/938], Loss: 0.0930\n",
            "Epoch [56/100], Step [200/938], Loss: 0.1386\n",
            "Epoch [56/100], Step [300/938], Loss: 0.1501\n",
            "Epoch [56/100], Step [400/938], Loss: 0.1582\n",
            "Epoch [56/100], Step [500/938], Loss: 0.1424\n",
            "Epoch [56/100], Step [600/938], Loss: 0.1441\n",
            "Epoch [56/100], Step [700/938], Loss: 0.0515\n",
            "Epoch [56/100], Step [800/938], Loss: 0.0869\n",
            "Epoch [56/100], Step [900/938], Loss: 0.1028\n",
            "Epoch [57/100], Step [100/938], Loss: 0.2148\n",
            "Epoch [57/100], Step [200/938], Loss: 0.1215\n",
            "Epoch [57/100], Step [300/938], Loss: 0.1293\n",
            "Epoch [57/100], Step [400/938], Loss: 0.2858\n",
            "Epoch [57/100], Step [500/938], Loss: 0.1417\n",
            "Epoch [57/100], Step [600/938], Loss: 0.0552\n",
            "Epoch [57/100], Step [700/938], Loss: 0.0799\n",
            "Epoch [57/100], Step [800/938], Loss: 0.0789\n",
            "Epoch [57/100], Step [900/938], Loss: 0.0572\n",
            "Epoch [58/100], Step [100/938], Loss: 0.1464\n",
            "Epoch [58/100], Step [200/938], Loss: 0.0649\n",
            "Epoch [58/100], Step [300/938], Loss: 0.1347\n",
            "Epoch [58/100], Step [400/938], Loss: 0.1726\n",
            "Epoch [58/100], Step [500/938], Loss: 0.2110\n",
            "Epoch [58/100], Step [600/938], Loss: 0.0735\n",
            "Epoch [58/100], Step [700/938], Loss: 0.1480\n",
            "Epoch [58/100], Step [800/938], Loss: 0.1215\n",
            "Epoch [58/100], Step [900/938], Loss: 0.1703\n",
            "Epoch [59/100], Step [100/938], Loss: 0.0608\n",
            "Epoch [59/100], Step [200/938], Loss: 0.0794\n",
            "Epoch [59/100], Step [300/938], Loss: 0.0763\n",
            "Epoch [59/100], Step [400/938], Loss: 0.1411\n",
            "Epoch [59/100], Step [500/938], Loss: 0.1620\n",
            "Epoch [59/100], Step [600/938], Loss: 0.0556\n",
            "Epoch [59/100], Step [700/938], Loss: 0.0553\n",
            "Epoch [59/100], Step [800/938], Loss: 0.2362\n",
            "Epoch [59/100], Step [900/938], Loss: 0.1313\n",
            "Epoch [60/100], Step [100/938], Loss: 0.0934\n",
            "Epoch [60/100], Step [200/938], Loss: 0.0464\n",
            "Epoch [60/100], Step [300/938], Loss: 0.0691\n",
            "Epoch [60/100], Step [400/938], Loss: 0.0979\n",
            "Epoch [60/100], Step [500/938], Loss: 0.0698\n",
            "Epoch [60/100], Step [600/938], Loss: 0.2375\n",
            "Epoch [60/100], Step [700/938], Loss: 0.0709\n",
            "Epoch [60/100], Step [800/938], Loss: 0.1556\n",
            "Epoch [60/100], Step [900/938], Loss: 0.0733\n",
            "Epoch [61/100], Step [100/938], Loss: 0.1302\n",
            "Epoch [61/100], Step [200/938], Loss: 0.0602\n",
            "Epoch [61/100], Step [300/938], Loss: 0.1553\n",
            "Epoch [61/100], Step [400/938], Loss: 0.2319\n",
            "Epoch [61/100], Step [500/938], Loss: 0.2378\n",
            "Epoch [61/100], Step [600/938], Loss: 0.1078\n",
            "Epoch [61/100], Step [700/938], Loss: 0.0571\n",
            "Epoch [61/100], Step [800/938], Loss: 0.1333\n",
            "Epoch [61/100], Step [900/938], Loss: 0.1824\n",
            "Epoch [62/100], Step [100/938], Loss: 0.1134\n",
            "Epoch [62/100], Step [200/938], Loss: 0.0376\n",
            "Epoch [62/100], Step [300/938], Loss: 0.0741\n",
            "Epoch [62/100], Step [400/938], Loss: 0.1659\n",
            "Epoch [62/100], Step [500/938], Loss: 0.0804\n",
            "Epoch [62/100], Step [600/938], Loss: 0.0887\n",
            "Epoch [62/100], Step [700/938], Loss: 0.1401\n",
            "Epoch [62/100], Step [800/938], Loss: 0.1358\n",
            "Epoch [62/100], Step [900/938], Loss: 0.1300\n",
            "Epoch [63/100], Step [100/938], Loss: 0.1875\n",
            "Epoch [63/100], Step [200/938], Loss: 0.1754\n",
            "Epoch [63/100], Step [300/938], Loss: 0.2488\n",
            "Epoch [63/100], Step [400/938], Loss: 0.0546\n",
            "Epoch [63/100], Step [500/938], Loss: 0.1527\n",
            "Epoch [63/100], Step [600/938], Loss: 0.1385\n",
            "Epoch [63/100], Step [700/938], Loss: 0.1408\n",
            "Epoch [63/100], Step [800/938], Loss: 0.2475\n",
            "Epoch [63/100], Step [900/938], Loss: 0.2230\n",
            "Epoch [64/100], Step [100/938], Loss: 0.0302\n",
            "Epoch [64/100], Step [200/938], Loss: 0.0333\n",
            "Epoch [64/100], Step [300/938], Loss: 0.1652\n",
            "Epoch [64/100], Step [400/938], Loss: 0.0843\n",
            "Epoch [64/100], Step [500/938], Loss: 0.0565\n",
            "Epoch [64/100], Step [600/938], Loss: 0.0524\n",
            "Epoch [64/100], Step [700/938], Loss: 0.3095\n",
            "Epoch [64/100], Step [800/938], Loss: 0.1580\n",
            "Epoch [64/100], Step [900/938], Loss: 0.0401\n",
            "Epoch [65/100], Step [100/938], Loss: 0.0735\n",
            "Epoch [65/100], Step [200/938], Loss: 0.1152\n",
            "Epoch [65/100], Step [300/938], Loss: 0.1284\n",
            "Epoch [65/100], Step [400/938], Loss: 0.0427\n",
            "Epoch [65/100], Step [500/938], Loss: 0.1707\n",
            "Epoch [65/100], Step [600/938], Loss: 0.1182\n",
            "Epoch [65/100], Step [700/938], Loss: 0.1544\n",
            "Epoch [65/100], Step [800/938], Loss: 0.1955\n",
            "Epoch [65/100], Step [900/938], Loss: 0.0818\n",
            "Epoch [66/100], Step [100/938], Loss: 0.0709\n",
            "Epoch [66/100], Step [200/938], Loss: 0.0882\n",
            "Epoch [66/100], Step [300/938], Loss: 0.1699\n",
            "Epoch [66/100], Step [400/938], Loss: 0.0694\n",
            "Epoch [66/100], Step [500/938], Loss: 0.1760\n",
            "Epoch [66/100], Step [600/938], Loss: 0.1394\n",
            "Epoch [66/100], Step [700/938], Loss: 0.0851\n",
            "Epoch [66/100], Step [800/938], Loss: 0.1764\n",
            "Epoch [66/100], Step [900/938], Loss: 0.0495\n",
            "Epoch [67/100], Step [100/938], Loss: 0.1685\n",
            "Epoch [67/100], Step [200/938], Loss: 0.0816\n",
            "Epoch [67/100], Step [300/938], Loss: 0.2590\n",
            "Epoch [67/100], Step [400/938], Loss: 0.1107\n",
            "Epoch [67/100], Step [500/938], Loss: 0.1750\n",
            "Epoch [67/100], Step [600/938], Loss: 0.0228\n",
            "Epoch [67/100], Step [700/938], Loss: 0.1562\n",
            "Epoch [67/100], Step [800/938], Loss: 0.1196\n",
            "Epoch [67/100], Step [900/938], Loss: 0.1314\n",
            "Epoch [68/100], Step [100/938], Loss: 0.0977\n",
            "Epoch [68/100], Step [200/938], Loss: 0.1687\n",
            "Epoch [68/100], Step [300/938], Loss: 0.1014\n",
            "Epoch [68/100], Step [400/938], Loss: 0.1321\n",
            "Epoch [68/100], Step [500/938], Loss: 0.1294\n",
            "Epoch [68/100], Step [600/938], Loss: 0.2656\n",
            "Epoch [68/100], Step [700/938], Loss: 0.2583\n",
            "Epoch [68/100], Step [800/938], Loss: 0.2412\n",
            "Epoch [68/100], Step [900/938], Loss: 0.1108\n",
            "Epoch [69/100], Step [100/938], Loss: 0.1242\n",
            "Epoch [69/100], Step [200/938], Loss: 0.2298\n",
            "Epoch [69/100], Step [300/938], Loss: 0.1197\n",
            "Epoch [69/100], Step [400/938], Loss: 0.1733\n",
            "Epoch [69/100], Step [500/938], Loss: 0.1198\n",
            "Epoch [69/100], Step [600/938], Loss: 0.0869\n",
            "Epoch [69/100], Step [700/938], Loss: 0.0482\n",
            "Epoch [69/100], Step [800/938], Loss: 0.0828\n",
            "Epoch [69/100], Step [900/938], Loss: 0.0578\n",
            "Epoch [70/100], Step [100/938], Loss: 0.1614\n",
            "Epoch [70/100], Step [200/938], Loss: 0.1659\n",
            "Epoch [70/100], Step [300/938], Loss: 0.1429\n",
            "Epoch [70/100], Step [400/938], Loss: 0.0647\n",
            "Epoch [70/100], Step [500/938], Loss: 0.0811\n",
            "Epoch [70/100], Step [600/938], Loss: 0.1086\n",
            "Epoch [70/100], Step [700/938], Loss: 0.2252\n",
            "Epoch [70/100], Step [800/938], Loss: 0.0807\n",
            "Epoch [70/100], Step [900/938], Loss: 0.0864\n",
            "Epoch [71/100], Step [100/938], Loss: 0.0844\n",
            "Epoch [71/100], Step [200/938], Loss: 0.2225\n",
            "Epoch [71/100], Step [300/938], Loss: 0.0744\n",
            "Epoch [71/100], Step [400/938], Loss: 0.0994\n",
            "Epoch [71/100], Step [500/938], Loss: 0.1246\n",
            "Epoch [71/100], Step [600/938], Loss: 0.0720\n",
            "Epoch [71/100], Step [700/938], Loss: 0.0926\n",
            "Epoch [71/100], Step [800/938], Loss: 0.1749\n",
            "Epoch [71/100], Step [900/938], Loss: 0.2650\n",
            "Epoch [72/100], Step [100/938], Loss: 0.0523\n",
            "Epoch [72/100], Step [200/938], Loss: 0.1176\n",
            "Epoch [72/100], Step [300/938], Loss: 0.0936\n",
            "Epoch [72/100], Step [400/938], Loss: 0.1017\n",
            "Epoch [72/100], Step [500/938], Loss: 0.0750\n",
            "Epoch [72/100], Step [600/938], Loss: 0.1358\n",
            "Epoch [72/100], Step [700/938], Loss: 0.0873\n",
            "Epoch [72/100], Step [800/938], Loss: 0.1161\n",
            "Epoch [72/100], Step [900/938], Loss: 0.0807\n",
            "Epoch [73/100], Step [100/938], Loss: 0.0370\n",
            "Epoch [73/100], Step [200/938], Loss: 0.0629\n",
            "Epoch [73/100], Step [300/938], Loss: 0.1901\n",
            "Epoch [73/100], Step [400/938], Loss: 0.0840\n",
            "Epoch [73/100], Step [500/938], Loss: 0.0574\n",
            "Epoch [73/100], Step [600/938], Loss: 0.1427\n",
            "Epoch [73/100], Step [700/938], Loss: 0.1464\n",
            "Epoch [73/100], Step [800/938], Loss: 0.0970\n",
            "Epoch [73/100], Step [900/938], Loss: 0.0700\n",
            "Epoch [74/100], Step [100/938], Loss: 0.1155\n",
            "Epoch [74/100], Step [200/938], Loss: 0.1180\n",
            "Epoch [74/100], Step [300/938], Loss: 0.0900\n",
            "Epoch [74/100], Step [400/938], Loss: 0.1641\n",
            "Epoch [74/100], Step [500/938], Loss: 0.0181\n",
            "Epoch [74/100], Step [600/938], Loss: 0.1680\n",
            "Epoch [74/100], Step [700/938], Loss: 0.0732\n",
            "Epoch [74/100], Step [800/938], Loss: 0.0369\n",
            "Epoch [74/100], Step [900/938], Loss: 0.0327\n",
            "Epoch [75/100], Step [100/938], Loss: 0.0415\n",
            "Epoch [75/100], Step [200/938], Loss: 0.0345\n",
            "Epoch [75/100], Step [300/938], Loss: 0.2341\n",
            "Epoch [75/100], Step [400/938], Loss: 0.0535\n",
            "Epoch [75/100], Step [500/938], Loss: 0.0930\n",
            "Epoch [75/100], Step [600/938], Loss: 0.1831\n",
            "Epoch [75/100], Step [700/938], Loss: 0.0946\n",
            "Epoch [75/100], Step [800/938], Loss: 0.0518\n",
            "Epoch [75/100], Step [900/938], Loss: 0.0892\n",
            "Epoch [76/100], Step [100/938], Loss: 0.0764\n",
            "Epoch [76/100], Step [200/938], Loss: 0.3408\n",
            "Epoch [76/100], Step [300/938], Loss: 0.0884\n",
            "Epoch [76/100], Step [400/938], Loss: 0.0604\n",
            "Epoch [76/100], Step [500/938], Loss: 0.0850\n",
            "Epoch [76/100], Step [600/938], Loss: 0.1616\n",
            "Epoch [76/100], Step [700/938], Loss: 0.1153\n",
            "Epoch [76/100], Step [800/938], Loss: 0.0867\n",
            "Epoch [76/100], Step [900/938], Loss: 0.1836\n",
            "Epoch [77/100], Step [100/938], Loss: 0.1075\n",
            "Epoch [77/100], Step [200/938], Loss: 0.0800\n",
            "Epoch [77/100], Step [300/938], Loss: 0.0514\n",
            "Epoch [77/100], Step [400/938], Loss: 0.0977\n",
            "Epoch [77/100], Step [500/938], Loss: 0.1740\n",
            "Epoch [77/100], Step [600/938], Loss: 0.2497\n",
            "Epoch [77/100], Step [700/938], Loss: 0.0402\n",
            "Epoch [77/100], Step [800/938], Loss: 0.0580\n",
            "Epoch [77/100], Step [900/938], Loss: 0.1702\n",
            "Epoch [78/100], Step [100/938], Loss: 0.2209\n",
            "Epoch [78/100], Step [200/938], Loss: 0.0544\n",
            "Epoch [78/100], Step [300/938], Loss: 0.2218\n",
            "Epoch [78/100], Step [400/938], Loss: 0.1008\n",
            "Epoch [78/100], Step [500/938], Loss: 0.1471\n",
            "Epoch [78/100], Step [600/938], Loss: 0.1194\n",
            "Epoch [78/100], Step [700/938], Loss: 0.1058\n",
            "Epoch [78/100], Step [800/938], Loss: 0.0783\n",
            "Epoch [78/100], Step [900/938], Loss: 0.0605\n",
            "Epoch [79/100], Step [100/938], Loss: 0.1246\n",
            "Epoch [79/100], Step [200/938], Loss: 0.0688\n",
            "Epoch [79/100], Step [300/938], Loss: 0.0803\n",
            "Epoch [79/100], Step [400/938], Loss: 0.1295\n",
            "Epoch [79/100], Step [500/938], Loss: 0.1149\n",
            "Epoch [79/100], Step [600/938], Loss: 0.1492\n",
            "Epoch [79/100], Step [700/938], Loss: 0.0873\n",
            "Epoch [79/100], Step [800/938], Loss: 0.0625\n",
            "Epoch [79/100], Step [900/938], Loss: 0.0691\n",
            "Epoch [80/100], Step [100/938], Loss: 0.0745\n",
            "Epoch [80/100], Step [200/938], Loss: 0.0459\n",
            "Epoch [80/100], Step [300/938], Loss: 0.0860\n",
            "Epoch [80/100], Step [400/938], Loss: 0.1307\n",
            "Epoch [80/100], Step [500/938], Loss: 0.1260\n",
            "Epoch [80/100], Step [600/938], Loss: 0.0596\n",
            "Epoch [80/100], Step [700/938], Loss: 0.1555\n",
            "Epoch [80/100], Step [800/938], Loss: 0.0321\n",
            "Epoch [80/100], Step [900/938], Loss: 0.1227\n",
            "Epoch [81/100], Step [100/938], Loss: 0.1096\n",
            "Epoch [81/100], Step [200/938], Loss: 0.1004\n",
            "Epoch [81/100], Step [300/938], Loss: 0.0937\n",
            "Epoch [81/100], Step [400/938], Loss: 0.0679\n",
            "Epoch [81/100], Step [500/938], Loss: 0.0435\n",
            "Epoch [81/100], Step [600/938], Loss: 0.1337\n",
            "Epoch [81/100], Step [700/938], Loss: 0.1065\n",
            "Epoch [81/100], Step [800/938], Loss: 0.0807\n",
            "Epoch [81/100], Step [900/938], Loss: 0.0550\n",
            "Epoch [82/100], Step [100/938], Loss: 0.1409\n",
            "Epoch [82/100], Step [200/938], Loss: 0.0645\n",
            "Epoch [82/100], Step [300/938], Loss: 0.1221\n",
            "Epoch [82/100], Step [400/938], Loss: 0.0586\n",
            "Epoch [82/100], Step [500/938], Loss: 0.1433\n",
            "Epoch [82/100], Step [600/938], Loss: 0.1720\n",
            "Epoch [82/100], Step [700/938], Loss: 0.0781\n",
            "Epoch [82/100], Step [800/938], Loss: 0.1551\n",
            "Epoch [82/100], Step [900/938], Loss: 0.0926\n",
            "Epoch [83/100], Step [100/938], Loss: 0.0964\n",
            "Epoch [83/100], Step [200/938], Loss: 0.1406\n",
            "Epoch [83/100], Step [300/938], Loss: 0.0890\n",
            "Epoch [83/100], Step [400/938], Loss: 0.1710\n",
            "Epoch [83/100], Step [500/938], Loss: 0.0986\n",
            "Epoch [83/100], Step [600/938], Loss: 0.1012\n",
            "Epoch [83/100], Step [700/938], Loss: 0.0326\n",
            "Epoch [83/100], Step [800/938], Loss: 0.1373\n",
            "Epoch [83/100], Step [900/938], Loss: 0.0399\n",
            "Epoch [84/100], Step [100/938], Loss: 0.1512\n",
            "Epoch [84/100], Step [200/938], Loss: 0.0887\n",
            "Epoch [84/100], Step [300/938], Loss: 0.0795\n",
            "Epoch [84/100], Step [400/938], Loss: 0.1053\n",
            "Epoch [84/100], Step [500/938], Loss: 0.0942\n",
            "Epoch [84/100], Step [600/938], Loss: 0.0886\n",
            "Epoch [84/100], Step [700/938], Loss: 0.0316\n",
            "Epoch [84/100], Step [800/938], Loss: 0.0938\n",
            "Epoch [84/100], Step [900/938], Loss: 0.0395\n",
            "Epoch [85/100], Step [100/938], Loss: 0.0792\n",
            "Epoch [85/100], Step [200/938], Loss: 0.0358\n",
            "Epoch [85/100], Step [300/938], Loss: 0.0932\n",
            "Epoch [85/100], Step [400/938], Loss: 0.0926\n",
            "Epoch [85/100], Step [500/938], Loss: 0.0684\n",
            "Epoch [85/100], Step [600/938], Loss: 0.1557\n",
            "Epoch [85/100], Step [700/938], Loss: 0.0732\n",
            "Epoch [85/100], Step [800/938], Loss: 0.1198\n",
            "Epoch [85/100], Step [900/938], Loss: 0.0240\n",
            "Epoch [86/100], Step [100/938], Loss: 0.1182\n",
            "Epoch [86/100], Step [200/938], Loss: 0.0839\n",
            "Epoch [86/100], Step [300/938], Loss: 0.1006\n",
            "Epoch [86/100], Step [400/938], Loss: 0.1167\n",
            "Epoch [86/100], Step [500/938], Loss: 0.0996\n",
            "Epoch [86/100], Step [600/938], Loss: 0.0853\n",
            "Epoch [86/100], Step [700/938], Loss: 0.1023\n",
            "Epoch [86/100], Step [800/938], Loss: 0.0230\n",
            "Epoch [86/100], Step [900/938], Loss: 0.1951\n",
            "Epoch [87/100], Step [100/938], Loss: 0.2579\n",
            "Epoch [87/100], Step [200/938], Loss: 0.1192\n",
            "Epoch [87/100], Step [300/938], Loss: 0.1229\n",
            "Epoch [87/100], Step [400/938], Loss: 0.1766\n",
            "Epoch [87/100], Step [500/938], Loss: 0.0553\n",
            "Epoch [87/100], Step [600/938], Loss: 0.0852\n",
            "Epoch [87/100], Step [700/938], Loss: 0.1818\n",
            "Epoch [87/100], Step [800/938], Loss: 0.0569\n",
            "Epoch [87/100], Step [900/938], Loss: 0.0947\n",
            "Epoch [88/100], Step [100/938], Loss: 0.1580\n",
            "Epoch [88/100], Step [200/938], Loss: 0.1703\n",
            "Epoch [88/100], Step [300/938], Loss: 0.0956\n",
            "Epoch [88/100], Step [400/938], Loss: 0.0730\n",
            "Epoch [88/100], Step [500/938], Loss: 0.1014\n",
            "Epoch [88/100], Step [600/938], Loss: 0.1667\n",
            "Epoch [88/100], Step [700/938], Loss: 0.0451\n",
            "Epoch [88/100], Step [800/938], Loss: 0.0487\n",
            "Epoch [88/100], Step [900/938], Loss: 0.1180\n",
            "Epoch [89/100], Step [100/938], Loss: 0.1431\n",
            "Epoch [89/100], Step [200/938], Loss: 0.0927\n",
            "Epoch [89/100], Step [300/938], Loss: 0.1407\n",
            "Epoch [89/100], Step [400/938], Loss: 0.1058\n",
            "Epoch [89/100], Step [500/938], Loss: 0.0295\n",
            "Epoch [89/100], Step [600/938], Loss: 0.0833\n",
            "Epoch [89/100], Step [700/938], Loss: 0.0691\n",
            "Epoch [89/100], Step [800/938], Loss: 0.0445\n",
            "Epoch [89/100], Step [900/938], Loss: 0.1475\n",
            "Epoch [90/100], Step [100/938], Loss: 0.1515\n",
            "Epoch [90/100], Step [200/938], Loss: 0.2159\n",
            "Epoch [90/100], Step [300/938], Loss: 0.0523\n",
            "Epoch [90/100], Step [400/938], Loss: 0.0884\n",
            "Epoch [90/100], Step [500/938], Loss: 0.0392\n",
            "Epoch [90/100], Step [600/938], Loss: 0.0916\n",
            "Epoch [90/100], Step [700/938], Loss: 0.1261\n",
            "Epoch [90/100], Step [800/938], Loss: 0.0774\n",
            "Epoch [90/100], Step [900/938], Loss: 0.0463\n",
            "Epoch [91/100], Step [100/938], Loss: 0.0482\n",
            "Epoch [91/100], Step [200/938], Loss: 0.0663\n",
            "Epoch [91/100], Step [300/938], Loss: 0.2108\n",
            "Epoch [91/100], Step [400/938], Loss: 0.0914\n",
            "Epoch [91/100], Step [500/938], Loss: 0.1063\n",
            "Epoch [91/100], Step [600/938], Loss: 0.0684\n",
            "Epoch [91/100], Step [700/938], Loss: 0.0343\n",
            "Epoch [91/100], Step [800/938], Loss: 0.0693\n",
            "Epoch [91/100], Step [900/938], Loss: 0.0604\n",
            "Epoch [92/100], Step [100/938], Loss: 0.0770\n",
            "Epoch [92/100], Step [200/938], Loss: 0.0523\n",
            "Epoch [92/100], Step [300/938], Loss: 0.0605\n",
            "Epoch [92/100], Step [400/938], Loss: 0.0329\n",
            "Epoch [92/100], Step [500/938], Loss: 0.1331\n",
            "Epoch [92/100], Step [600/938], Loss: 0.1054\n",
            "Epoch [92/100], Step [700/938], Loss: 0.0470\n",
            "Epoch [92/100], Step [800/938], Loss: 0.0486\n",
            "Epoch [92/100], Step [900/938], Loss: 0.1437\n",
            "Epoch [93/100], Step [100/938], Loss: 0.1130\n",
            "Epoch [93/100], Step [200/938], Loss: 0.0344\n",
            "Epoch [93/100], Step [300/938], Loss: 0.0891\n",
            "Epoch [93/100], Step [400/938], Loss: 0.2166\n",
            "Epoch [93/100], Step [500/938], Loss: 0.0460\n",
            "Epoch [93/100], Step [600/938], Loss: 0.1242\n",
            "Epoch [93/100], Step [700/938], Loss: 0.0608\n",
            "Epoch [93/100], Step [800/938], Loss: 0.1453\n",
            "Epoch [93/100], Step [900/938], Loss: 0.1578\n",
            "Epoch [94/100], Step [100/938], Loss: 0.0516\n",
            "Epoch [94/100], Step [200/938], Loss: 0.0545\n",
            "Epoch [94/100], Step [300/938], Loss: 0.0655\n",
            "Epoch [94/100], Step [400/938], Loss: 0.1331\n",
            "Epoch [94/100], Step [500/938], Loss: 0.0483\n",
            "Epoch [94/100], Step [600/938], Loss: 0.1043\n",
            "Epoch [94/100], Step [700/938], Loss: 0.1136\n",
            "Epoch [94/100], Step [800/938], Loss: 0.1031\n",
            "Epoch [94/100], Step [900/938], Loss: 0.0632\n",
            "Epoch [95/100], Step [100/938], Loss: 0.0500\n",
            "Epoch [95/100], Step [200/938], Loss: 0.1053\n",
            "Epoch [95/100], Step [300/938], Loss: 0.0750\n",
            "Epoch [95/100], Step [400/938], Loss: 0.0815\n",
            "Epoch [95/100], Step [500/938], Loss: 0.1234\n",
            "Epoch [95/100], Step [600/938], Loss: 0.0260\n",
            "Epoch [95/100], Step [700/938], Loss: 0.0158\n",
            "Epoch [95/100], Step [800/938], Loss: 0.1280\n",
            "Epoch [95/100], Step [900/938], Loss: 0.0945\n",
            "Epoch [96/100], Step [100/938], Loss: 0.0197\n",
            "Epoch [96/100], Step [200/938], Loss: 0.0482\n",
            "Epoch [96/100], Step [300/938], Loss: 0.0191\n",
            "Epoch [96/100], Step [400/938], Loss: 0.0212\n",
            "Epoch [96/100], Step [500/938], Loss: 0.0558\n",
            "Epoch [96/100], Step [600/938], Loss: 0.0436\n",
            "Epoch [96/100], Step [700/938], Loss: 0.1334\n",
            "Epoch [96/100], Step [800/938], Loss: 0.0655\n",
            "Epoch [96/100], Step [900/938], Loss: 0.1189\n",
            "Epoch [97/100], Step [100/938], Loss: 0.0295\n",
            "Epoch [97/100], Step [200/938], Loss: 0.0835\n",
            "Epoch [97/100], Step [300/938], Loss: 0.1677\n",
            "Epoch [97/100], Step [400/938], Loss: 0.0485\n",
            "Epoch [97/100], Step [500/938], Loss: 0.0426\n",
            "Epoch [97/100], Step [600/938], Loss: 0.0714\n",
            "Epoch [97/100], Step [700/938], Loss: 0.0640\n",
            "Epoch [97/100], Step [800/938], Loss: 0.0632\n",
            "Epoch [97/100], Step [900/938], Loss: 0.0638\n",
            "Epoch [98/100], Step [100/938], Loss: 0.0808\n",
            "Epoch [98/100], Step [200/938], Loss: 0.0904\n",
            "Epoch [98/100], Step [300/938], Loss: 0.0302\n",
            "Epoch [98/100], Step [400/938], Loss: 0.1773\n",
            "Epoch [98/100], Step [500/938], Loss: 0.0698\n",
            "Epoch [98/100], Step [600/938], Loss: 0.0779\n",
            "Epoch [98/100], Step [700/938], Loss: 0.1086\n",
            "Epoch [98/100], Step [800/938], Loss: 0.0999\n",
            "Epoch [98/100], Step [900/938], Loss: 0.0267\n",
            "Epoch [99/100], Step [100/938], Loss: 0.0986\n",
            "Epoch [99/100], Step [200/938], Loss: 0.0826\n",
            "Epoch [99/100], Step [300/938], Loss: 0.2112\n",
            "Epoch [99/100], Step [400/938], Loss: 0.0840\n",
            "Epoch [99/100], Step [500/938], Loss: 0.1400\n",
            "Epoch [99/100], Step [600/938], Loss: 0.0208\n",
            "Epoch [99/100], Step [700/938], Loss: 0.0929\n",
            "Epoch [99/100], Step [800/938], Loss: 0.2118\n",
            "Epoch [99/100], Step [900/938], Loss: 0.1154\n",
            "Epoch [100/100], Step [100/938], Loss: 0.0394\n",
            "Epoch [100/100], Step [200/938], Loss: 0.1900\n",
            "Epoch [100/100], Step [300/938], Loss: 0.1139\n",
            "Epoch [100/100], Step [400/938], Loss: 0.1406\n",
            "Epoch [100/100], Step [500/938], Loss: 0.0264\n",
            "Epoch [100/100], Step [600/938], Loss: 0.1736\n",
            "Epoch [100/100], Step [700/938], Loss: 0.0500\n",
            "Epoch [100/100], Step [800/938], Loss: 0.0759\n",
            "Epoch [100/100], Step [900/938], Loss: 0.1308\n",
            "Accuracy of the network on the 10000 test images: 96.47 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "clf_1 = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf_1.fit(X_train, y_train)\n",
        "\n",
        "y_pred_1 = clf_1.predict(X_test)\n",
        "\n",
        "accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
        "print(\"SVM Accuracy: \" + str(accuracy_1 * 100) + \" %\")\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "clf_2 = tree.DecisionTreeClassifier(max_depth=10)\n",
        "clf_2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_2 = clf_2.predict(X_test)\n",
        "\n",
        "accuracy_2 = accuracy_score(y_test, y_pred_2)\n",
        "print(\"Decision Tree Accuracy: \" + str(accuracy_2 * 100) + \" %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn2NJ96OeKV7",
        "outputId": "f6722550-6fa6-4542-c196-4f60ee200e55"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 97.35000000000001 %\n",
            "Decision Tree Accuracy: 79.08 %\n"
          ]
        }
      ]
    }
  ]
}